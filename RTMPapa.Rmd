---
title             : "Real-Time Modulation Perception in Western Classical Music"
shorttitle        : "Modulation Perception in Western Music"

author: 
  - name          : "Brendon Mizener"
    affiliation   : "1"
    address       : "800 W. Campbell Rd., GR41, Richardson, TX 75080"
    corresponding : yes # Define only one corresponding author
    email         : "bmizener@utdallas.edu"
  - name          : "W. Jay Dowling"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Texas at Dallas"

authornote: |

   Music Perception and Cognition Lab, Dr. W. Jay Dowling, Director. School of Brain and Behavioral Sciences, University of Texas at Dallas.

abstract: |

  During music listening The auditory scene analysis performed during is an incredibly complex process. The task of listening involves the listener making many judgments per second. Those judgments relate to melody, timing, and key or tonic region. Here we examine whether the process of tracking key region is independent of the process of tracking surface cues, and what surface cues may influence that process. To this end, highly-trained, moderately-trained, and untrained listeners were asked to respond to excerpts from string quartets, quintets, and sextets from the classical and romantic eras and react when and if they heard a modulation. Each excerpt featured either a pivot chord modulation, a direct modulation, a common tone modulation, or no modulation. Responses were analyzed using *A*' and reaction time. Listeners perform above chance across all training levels and modulation conditions, with musical features including modulation type, time, and mode change, as well as participant training level being significant factors in overall accuracy.    

  
keywords          : "Modulation, Tonic Area, Music Training, Reaction Time"
wordcount         : "11161"

bibliography      : C:\\Users\\Brendon\\Documents\\R Stuff\\Bibtex\\Modulation Study.bib

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes   :
      - \usepackage{musicography}
      - \usepackage{tabu}
      - \raggedbottom
      - \usepackage{float}

#appendix:
#  - "appendix_a.Rmd"
#  - "appendix_b.Rmd"

---

```{r setup, include = FALSE}
library("papaja")

```



```{r packages, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

##  Libraries ----

library(openxlsx)
library(tibble)
library(reshape2)
library(randomcoloR)
library(car)
library(ggpubr)
library(ggplot2)
library(olsrr)
library("papaja")
library("kableExtra")
library("citr")
library("plyr")
library("dplyr")
```



```{r stimdatain, include = FALSE}
library("car")

# Get stimulus data: stimdatswap = "Stim data with A'", i.e. A' has already been calculated, and add headers. 

stimdatswap <- read.csv("stimdatswaps.csv", header = FALSE)
names(stimdatswap) <- c("stimno", "mod_type", "Key_dist", "mode_change", "hits", "hitr", "FAs", "FAr", "Apr")
rtsub <- read.xlsx("Participant_Data.xlsx", rows = 1:181, cols = 1:4) # reaction time by subject.
# This is a test scatter plot that includes a smooth line (?) (I'm not super sure what the "smooth" is for or what the line is supposed to represent)

scatter.smooth(x=stimdatswap$Key_dist, y=stimdatswap$Apr, main="A' by Key Distance")

# Regression of Key distance predicting A'
pd <- position_dodge(0.3) # initializing now so that it's usable throughout the entire script.


Aprbykey <- lm(Apr ~ Key_dist, data = stimdatswap[1:42, ])
kdss <- Anova(Aprbykey, type = 3)

aprbykey_plot <- ggplot(data = stimdatswap, aes(x = Key_dist, y = Apr)) +
                          geom_point(aes(x = Key_dist, y = Apr)) +
                          stat_smooth(method=lm)

aprbykey_plot2 <- ggplot(data = stimdatswap[1:42, ], aes(x = Key_dist, y = Apr)) +
                          geom_point(aes(x = Key_dist, y = Apr)) +
                          stat_smooth(method=lm)
                            


# ANOVA on A', grouped by mode change
test <- aov(Apr ~ mode_change, data = stimdatswap)
v <- aov(Key_dist ~ mod_type, data = stimdatswap)
v.s <- apa_print(v)
# run model again coding mode change as -1 and 1 (use effects coding), then switch 

# Multiple regression of Key distance and mode change predicting A'

# add column and code for effects codes
stimdatswap <- add_column(stimdatswap, mode_change_ec = stimdatswap$mode_change, .after = "mode_change")
stimdatswap$mode_change_ec[stimdatswap$mode_change == 0] <- -1

#add column and code as y/n
stimdatswap <- add_column(stimdatswap, "Mode_Change_yn"= stimdatswap$mode_change, .after = "mode_change_ec")
stimdatswap$'Mode_Change_yn'[stimdatswap$mode_change == 0] <- "no"
stimdatswap$'Mode_Change_yn'[stimdatswap$mode_change == 1] <- "yes"

#add column and reverse code
stimdatswap <- add_column(stimdatswap, mode_change_rc = stimdatswap$mode_change, .after = "Mode_Change_yn")
stimdatswap$mode_change_rc[stimdatswap$mode_change == 0] <- 1
stimdatswap$mode_change_rc[stimdatswap$mode_change == 1] <- 0

Aprbykey.mcec <- lm(Apr ~ Key_dist * mode_change_ec, data = stimdatswap[1:42, ])


#Next section's code comes from the R Graphics Cookbook (pp 94 - 99), by Winston Chang

make_model <- function(data) {
  lm(Apr ~ Key_dist, data)
}

predictvals <- function(model, xvar, yvar, xrange=NULL, samples = 100, ...){
  if (is.null(xrange)) {
    if (any(class(model) %in% c("lm", "glm")))
      xrange <- range(model$model[[xvar]])
    else if (any(class(model) %in% "loess"))
      xrange <- range(model$x)
  }
  
  newdata <- data.frame(x = seq(xrange[1], xrange[2], length.out = samples))
  names(newdata) <- xvar
  newdata[[yvar]] <- predict(model, newdata = newdata, ...)
  newdata
}

sdordered <- stimdatswap[1:42, ]
sdordered <- sdordered[order(sdordered$mode_change), ]


models <- dlply(.data = stimdatswap[1:42, ], .variables = "Mode_Change_yn", .fun = make_model)
predvals <- ldply(.data = models, .fun = predictvals, xvar = "Key_dist", yvar = "Apr")

abkmcecp <- ggplot(data = stimdatswap[1:42, ], aes(x = Key_dist, y = Apr, color = Mode_Change_yn, shape = Mode_Change_yn)) +
                   geom_point() +
    #geom_smooth(method = lm, se = FALSE ) +              
   geom_line(data = predvals, size = 1) +
   theme_bw() +
   theme(legend.position ="right") +
   xlab("Key Distance") +
   ylab("A'") +
   labs(color = "Mode Change", shape = "Mode Change")
   #ggtitle("Multiple Regression using Key Distance and Mode Change as Predictors")

bwcols <- c("gray12", "gray52")

bwcols <- matrix(nrow = 42, ncol = 1)
bwcols[ ,1] <- stimdatswap$mode_change[1:42]
bwcols[which(bwcols[ ,1]==1)] <- ("gray12")
bwcols[which(bwcols[ ,1]==0)] <- ("gray52")

abkmcecp_bw <- ggplot(data = stimdatswap[1:42, ], 
                      aes(x = Key_dist, y = Apr, color = Mode_Change_yn, 
                          shape = Mode_Change_yn)) +
                      geom_point(size = 2.5, position = "jitter") +
                      geom_line(data = predvals, size = 1) +
                      theme_apa() +
                      scale_color_grey() +
                      theme(legend.position ="right") +
                      xlab("Key Distance") +
                      ylab("A'") +
                      labs(color = "Mode Change", shape = "Mode Change")
                   



Aprbykey.mcdc <- lm(Apr ~ Key_dist * mode_change, data = stimdatswap[1:42, ])
Aprbykey.mcrc <- lm(Apr ~ Key_dist * mode_change_rc, data = stimdatswap[1:42, ])

#that section ends here


kmss <- Anova(Aprbykey, type = 3, )

# ANOVA of Mode Change (2) on A'

Aprbymc <- aov(Apr ~ mode_change, data = stimdatswap)

ssmc <- Anova(Aprbymc, type = 3)

#Anova of Modulation type (3) on A'

Aprbymt <- lm(Apr ~ mod_type, data = stimdatswap)

mtss <- Anova(Aprbymt, type = 3)


stimtemp <- stimdatswap[c(1:42) , c(1,12)]
tempodata <- read.csv("rtonly.csv", header = TRUE)
stimtemp <- add_column(stimtemp, tempo = tempodata$Tempo[1:42], .after = "Apr")

tempolm <- summary(lm(data = stimtemp, formula = Apr ~ tempo))

```


```{r subdatain, include = FALSE}


## Analysis of Subject A' Data for 2019 Modulation Perception study (first year project UTD)----
## Brendon Mizener
## 2019 - 8 - 27
##
##
##
##


#   Import data. ----

allsubjects <- read.table("compiledcsvs.txt", header = TRUE) ## compiledcsvs.txt contains each participants' hits and false alarms for each stimulus
subjectdata <- read.xlsx("Participant_Data.xlsx", rows = 1:181, cols = 1:4) #contains participant data: age, training, etc.
aprimedata <- read.table("aprime_data.txt", ',', header = FALSE) #contains A' statistics for overall and each subset, and the hits and false alarms and rates for each.

#   Data cleaning steps ----
allsubjects <- data.frame(allsubjects) 
allsubjects <- allsubjects[order(allsubjects$type) , ] #sorts allsubjects by modulation type
aprimeonly <- aprimedata[ , 1:5] #selects only the A' scores for each participant
names(aprimeonly) <- c("subject#", "apr_ovr", "apr_pc", "apr_dm", "apr_ct")   #rename columns to reflect values
aprimeonly <- add_column(aprimeonly, Class = subjectdata$Class, .before = "apr_ovr") #adds participant class (1 = untrained, 2 = moderate, 3 = high) from subjectdata spreadsheet
aprimeonly$Class <- as.factor(aprimeonly$Class) #sets "Class" as a factor, so it can be used properly in the analysis
aprimelong <- melt(aprimeonly, id.vars = c('subject#', 'Class'), measure.vars = c('apr_ovr', 'apr_pc', 'apr_dm', 'apr_ct')) #melts data into 4 columns
names(aprimelong) <- c('subject', "class", "modtype", "aprime")
years <- aprimeonly
years <- add_column(aprimeonly, years = subjectdata$Years.of.formal.training, .before = "apr_ovr")
years <- add_column(years, age = subjectdata$Age, .after = "years")

c12 <- distinctColorPalette(k = 12, altCol = FALSE, runTsne = FALSE)
c3 <- distinctColorPalette(k = 3, altCol = FALSE, runTsne = FALSE)



#   Analyses ----

# Simple Training level ANOVA w/ Tukey Test
training.lm <- with(aprimelong[1:180, ], lm(aprime ~ class))
training.aov <- with(aprimelong[1:180, ], aov(aprime ~ class))
tukey.training <- TukeyHSD(training.aov)

# Simple modulation type ANOVA w/Tukey Test
type.lm <- with(aprimelong[181:720, ], lm(aprime ~ modtype))
type.aov <- with(aprimelong[181:720, ], aov(aprime ~ modtype))
tukey.type <- TukeyHSD(type.aov)

# Interaction model ANOVA w/Tukey Test
m.c.lm <- with(aprimelong[181:720, ], lm(aprime ~ modtype * class))
m.c.anova <- Anova(m.c.lm, type = 3)
m.c.aov <- with(aprimelong[181:720, ], aov(aprime ~ class * modtype))
tukeybyall <- TukeyHSD(m.c.aov)
cbym <- data.frame(tukeybyall$`class:modtype`)
sig.int <- data.frame(cbym$p.adj) < .05
sig.int <- cbym[sig.int, ]
#write.csv(x = sig.int, file = "sig_int.csv")


# Regression of years of training on overall score
y.lm <- with(years, lm(apr_ovr ~ years))
y.a.mr <- with(years, lm(apr_ovr ~ age + years))

# Test for influential data points
cdcheck <- with(aprimeonly, lm(apr_ovr ~ Class))
cdcheck.chart <- ols_plot_cooksd_chart(cdcheck)

# Post-hoc cleaning and analysis ----

icds <- c(2, 25, 26, 70, 157, 167) # subjects who were identified by cdcheck as influential data points
apocdo <- aprimeonly[-icds, ]      # dataframe of A' values with invfluential data points (identified as surpassing cook's distance) taken out 
summary(with(apocdo, aov(apr_ovr ~ Class)))
apocdo.long <- melt(apocdo,  id.vars = c('subject#', 'Class'), measure.vars = c('apr_ovr', 'apr_pc', 'apr_dm', 'apr_ct')) #melts data into 4 columns
names(apocdo.long) <- c('subject', "class", "modtype", "aprime")

# Anova of Cook's distances 
mtaov.cdo <- with(apocdo.long, aov(aprime ~ class * modtype))
tukeytmtcdo <- TukeyHSD(mtaov.cdo)


## Plots ----

t.aov.plot <- plot(training.aov) #plots generated by the training anova
m.aov.plot <- plot(type.aov) #plots generated by the modulation type
aprime.box <-ggboxplot(data = aprimelong, x = "class", facet.by = "modtype", y = "aprime", color = "class", label.select = list(top.down = 5) )

aprimeonly.2 <- aprimeonly[order(aprimeonly$Class), ]
aprimemeans <- matrix(c(mean(aprimeonly.2[1:60, 3]), mean(aprimeonly.2[61:120, 3]), mean(aprimeonly.2[121:180, 3]), 
                        mean(aprimeonly.2[1:60, 4]), mean(aprimeonly.2[61:120, 4]), mean(aprimeonly.2[121:180, 4]), 
                        mean(aprimeonly.2[1:60, 5]), mean(aprimeonly.2[61:120, 5]), mean(aprimeonly.2[121:180, 5]), 
                        mean(aprimeonly.2[1:60, 6]), mean(aprimeonly.2[61:120, 6]), mean(aprimeonly.2[121:180, 6])), nrow = 3, ncol = 4)
colnames(aprimemeans) <- c("ovr", "p", 'd', "c")
rownames(aprimemeans) <- c("low", "moderate", "high")
aprimemeans.m <- melt(aprimemeans, measure.vars = c("ovr", "p", "d", "c"))
colnames(aprimemeans.m) <- c("training", "modtype", "aprime")
aprimemeans.line <- ggplot(data = data.frame(aprimemeans.m[4:12, ]), aes(x = modtype, y = aprime, color = training)) + 
                      geom_boxplot(aes(color = training,  group = training), size = 1)
                      



#training.box <- ggboxplot(data = aprimeonly, x = "Class", y = "apr_ovr")
#training.dens <- ggdensity(aprimelong, x = "aprime", add = "mean", rug = TRUE, color = "class", 
#                           fill = "class", palette = c3)  
# modtype.aov <- with(allsub, aov(value ~ variable))
# modtype.box <- ggboxplot(allsub, x = "modtype", y = "aprime", color = "modtype", palette = c("#00AFBB", "#E7B800", "#FC4E07"))
# modtype.dens <- ggdensity(allsub, x = "aprime", add = "mean", rug = TRUE, color = "modtype", 
#                          fill = "modtype", palette = c("#00AFBB", "#E7B800", "#FC4E07"))  
  

#p2 <- ggplot(allsub.m) + geom_boxplot(aes(x = partclass, y = value, color = variable))
#p1 <- ggplot(allsub, aes(x = partclass, y.1 = pivot, y.2 = direct, y.3 = common)) + geom_boxplot()
#an <- aov(formula = partclass ~ pivot + direct + common, data = allsub, na.rm = TRUE)
#save.image(file = "C:\\Users\\Brendon\\Desktop\\Western\\Data\\plots.Rdata")




apocdo <- apocdo[order(apocdo$Class), ]
apocdo.means <- matrix(c(mean(apocdo[1:57, 3]), mean(apocdo[58:116, 3]), mean(apocdo[117:174, 3]), 
                         mean(apocdo[1:57, 4]), mean(apocdo[58:116, 4]), mean(apocdo[117:174, 4]), 
                         mean(apocdo[1:57, 5]), mean(apocdo[58:116, 5]), mean(apocdo[117:174, 5]), 
                         mean(apocdo[1:57, 6]), mean(apocdo[58:116, 6]), mean(apocdo[117:174, 6])), nrow = 3, ncol = 4)
colnames(apocdo.means) <- c("ovr", "p", 'd', "c")
rownames(apocdo.means) <- c("low", "moderate", "high")
apocdo.means.m <- melt(apocdo.means, measure.vars = c("ovr", "p", "d", "c"))
colnames(apocdo.means.m) <- c("training", "modtype", "aprime")
apocdo.means.line <- ggplot(data = data.frame(apocdo.means.m), aes(x = modtype, y = aprime, color = training)) + geom_line(aes(color = training, group = training), size = 1) + geom_point()




```

```{r Response Time, include = FALSE}




##    Import data----

rt <- read.table("rtonly.txt", header = TRUE)
rtsub <- read.xlsx("Participant_Data.xlsx", rows = 1:181, cols = 1:4)

##    Clean Data----

cols = c(1:5,186)
rtstim <- rt[1:49 , cols]
rtstim <- add_column(rtstim, apr = stimdatswap$Apr, .after = "rtmeanstim")

rtsub$Participant_ID = c(1:180)

rvec <- rt[50, 6:185]
rvec <- t(rvec)
rownames(rvec) <- rownames(rtsub)
rpcvec <- rt[51, 6:185]
rdmvec <- rt[52, 6:185]
rctvec <- rt[53, 6:185]
rpcvec <- t(rpcvec) 
rdmvec <- t(rdmvec) 
rctvec <- t(rctvec)
row.names(rpcvec) <- rownames(rtsub)
row.names(rdmvec) <- rownames(rtsub)
row.names(rctvec) <- rownames(rtsub)

rtsub <- add_column(rtsub, meanrt = rvec, .after = "Years.of.formal.training")
rtsub <- add_column(rtsub, pcrt = rpcvec, .after = "meanrt")
rtsub <- add_column(rtsub, dmrt = rdmvec, .after = "pcrt")
rtsub <- add_column(rtsub, ctrt = rctvec, .after = "dmrt")
colnames(rtsub) <- c("p.id", "age", "class", "y.t", "meanrt", "pcrt", "dmrt", "ctrt")
rtsub <- add_column(rtsub, apr_ovr = aprimedata$V2, .after = "ctrt")

rtsub$pcrt[rtsub$pcrt == 0] <- NA
rtsub$dmrt[rtsub$dmrt == 0] <- NA
rtsub$ctrt[rtsub$ctrt == 0] <- NA

rtsub$pcrt[is.nan(rtsub$pcrt)] <- NA
rtsub$dmrt[is.nan(rtsub$dmrt)] <- NA
rtsub$ctrt[is.nan(rtsub$ctrt)] <- NA

rtlong <- melt(rtsub, id.vars = c("p.id", "class"), measure.vars = c("pcrt", "dmrt", "ctrt", "meanrt"), value.name = "Reaction_Time", variable.name = "Modulation_Type" )

rtintaov <- with(rtlong, aov(Reaction_Time ~ Modulation_Type))

rtgroupmeans <- ddply(rtlong, c("class", "Modulation_Type"), summarise,
                N = length(Reaction_Time),
                mean = mean(Reaction_Time, na.rm = T),
                sd = sd(Reaction_Time, na.rm = T),
                se = sd/sqrt(N))  

group.rt.plot <- ggplot(data = rtgroupmeans, aes(x = Modulation_Type, y = mean,  color =  as.factor(class))) +
                  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, position = pd, size = 1) +
                  #geom_line(size = .5,  position = pd) +
                  geom_point(position = pd, size = 4, shape = 21, fill = "white") +
                  geom_text(aes(x = Modulation_Type, y = mean, label=round(mean, 2), hjust = 1.8)) +
                  xlab("Modulation type") +
                  ylab("Mean response time (s)") +
                  scale_colour_hue(name="Level of Training",
                                   breaks = c("1", "2", "3"),
                                   labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly trained (10+ yrs)"), 
                                   l=40) +
                    # ggtitle("Differences in response time by modulation type between groups") +
                    expand_limits(y = c(.5:1.1)) +
                    scale_y_continuous(breaks = c(.5, 1, 1.5, 2), labels = c(.5, 1, 1.5, 2)) +
                    scale_x_discrete(breaks = c("pcrt", "dmrt", "ctrt", "meanrt"), labels = c("Pivot chord", "Direct", "Common tone", "Overall mean"))+
                  labs(color = "Participant Training Level") +
                 # scale_x
                    theme_bw() +
                    theme(legend.justification = c(1,0), legend.position = c(1,0))

group.rt.apa <- ggplot(data = rtgroupmeans, aes(x = Modulation_Type, y = mean, fill = as.factor(class))) +
   geom_bar(color = "black", stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, size = 1, position = position_dodge(.9)) +
   geom_text(aes(x = Modulation_Type, y = mean, label=round(mean, 2)), position = position_dodge(.9), vjust = -4, color = "black", size = 2.5) + 
   scale_fill_grey(name="Level of Training", 
                   labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                        breaks = c("1", "2", "3"), ) +
   scale_x_discrete(breaks = c("pcrt", "dmrt", "ctrt", "meanrt"), labels = c("Pivot chord", "Direct", "Common \ntone", "Overall \nmean")) +
   theme_apa() + 
   coord_cartesian(ylim=c(0, 2.3)) +
   xlab("Modulation Type") +
   ylab("Mean response time (s)")
   
   

##    Analysis by Subject/Training Level----

subrt.aov <- aov(meanrt ~ factor(class), data = rtsub)
summary(subrt.aov)
subrt.tukey <- TukeyHSD(subrt.aov)
subrt.lm <- with(rtsub, lm(meanrt ~ class))
summary(subrt.lm)

rtap <- summary(lm(meanrt ~ apr_ovr, data = rtsub))

rtvaprsub <- ggplot(data = rtsub, mapping = aes(x = meanrt, y = apr_ovr)) +
                    geom_point(color = rtsub$class) + 
                    stat_smooth(method = "lm") +
                    theme_bw() +
                    theme(legend.justification = c(1,0), legend.position = c(1,0))
                    
                  

                


##    Analysis by Stimulus/Modulation type----

stimrt.aov <- with(rtstim[1:42, ], aov(rtmeanstim ~ factor(type)))
summary(stimrt.aov)
stimrt.tukey <- TukeyHSD(stimrt.aov)

##    Analysis by Stimulus and key distance----

stimrt.lm <- with(rtstim[1:42, ], lm(rtmeanstim ~ KeyDistance))
summary(stimrt.lm)


stimrt.lm.plot <- ggplot(data = rtstim[1:42, ], aes(x = KeyDistance, y = rtmeanstim)) +
                        geom_point() +
                        stat_smooth(method = "lm") +
                        ylab("Mean Response Time") +
                        xlab("Key Distance") +
   ggtitle("Response time by key distance") +
   scale_y_continuous(breaks = c(.5, 1.5, 2, 2.5, 3,3.5), labels = c(.5, 1.5, 2, 2.5, 3,3.5)) +
   scale_x_continuous(breaks = c(.5, 1, 1.5, 2), labels = c(.5, 1, 1.5, 2)) +
                        theme_bw() +
                        theme(legend.justification = c(1,0.1), legend.position = c(1,0.1))
  

##    Analysis of interactions----


## Plots

rtsubmeans <- ddply(rtsub, "class", summarise,
                N = length(meanrt),
                mean = mean(meanrt),
                sd = sd(meanrt),
                se = sd/sqrt(N))

sub.rt.plot <- ggplot(data = rtsubmeans, aes(x = class, y = mean,  color =  class)) +
                  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, position = pd, size = 1) +
                  #geom_line(size = .5,  position = pd) +
                  geom_point(position = pd, size = 4, shape = 21, fill = "white") +
                  geom_text(aes(x = class, y = mean, label=round(mean, 2), hjust = 1.5)) +
                  xlab("Listener training level") +
                  ylab("Mean response time") +
                  #scale_colour_hue(name="Modulation Type",
                                  # breaks = c("1", "2", "3"),
                                  # labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                                   #l=40) +
                    #ggtitle("Differences in response time by training level") +
                    expand_limits(y = c(.5:1.1)) +
                    scale_y_continuous(breaks = c(.5, 1, 1.5, 2), labels = c(.5, 1, 1.5, 2)) +
                    scale_x_continuous(breaks = c(1, 2, 3), labels = c(1, 2, 3)) +
                    theme_bw() +
                    theme(legend.position = "none")

pcfix <- rtstim$type == "1"
dmfix <- rtstim$type == "2"
ctfix <- rtstim$type == "3"
rtstim$type[pcfix] <- "1. Pivot Chord"
rtstim$type[dmfix] <- "2. Direct"
rtstim$type[ctfix] <- "3. Common Tone"

rtstimmeans <- ddply(rtstim[1:42, ], "type", summarise,
                N = length(rtmeanstim),
                mean = mean(rtmeanstim),
                sd = sd(rtmeanstim),
                se = sd/sqrt(N))

sub.rt.plot <- ggplot(data = rtstimmeans, aes(x = type, y = mean,  color =  type)) +
                  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, position = pd, size = 1) +
                  #geom_line(size = .5,  position = pd) +
                  geom_point(position = pd, size = 4, shape = 21, fill = "white") +
                  geom_text(aes(x = type, y = mean, label=round(mean, 2), hjust = 1.5)) +
                  xlab("Modulation type") +
                  ylab("Mean response time") +
                  #scale_colour_hue(name="Modulation Type",
                                  # breaks = c("1", "2", "3"),
                                  # labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                                   #l=40) +
                    #ggtitle("Differences in response time by modulation type") +
                    expand_limits(y = c(.5:1.1)) +
                    scale_y_continuous(breaks = c(.5, 1, 1.5, 2), labels = c(.5, 1, 1.5, 2)) +
                    theme_bw() +
                    theme(legend.position = "none")

```

```{r exploratory analyses, echo = F}


# Aprbykey <- add_column(rtsub, apr_ovr = aprimedata$V2, .after = "meanrt") #rtsub is reaction time by subject




```
  Listener understanding of pitch center, or tonic, is fundamental to understanding musical structures from the smallest musical motif to the large-scale structures that define musical form. The tonic in a musical phrase is the pitch that represents the foundation of that tonal hierarchy [@Krumhansl1979b]. Understanding of pitch as hierarchical has been well established [@Krumhansl1982c; @Lerdahl1983]. Listeners become aware of the tonic in a given composition through a number of musical and perceptual cues related to melody and melodic expectancy [@Brown1988; @Dowling1986; @Lerdahl1983] and harmony and phrase structure [@Huron1993; @Krumhansl1982a]. Additionally, it has been well established that music listeners implicitly understand the concept of a key and its tonic [@Dowling1978a; @Krumhansl1982c; @Krumhansl1979b], but much less work has been done on the understanding the perception of the motion between tonic centers within a single composition. This process of moving between key centers is called "modulation".  
  Modulation allows the composer to use a wider palette for musical expression. Western classical music theorists have defined many types of modulation, each of which is characterized by its own parameters and musical theoretical constructs.  One question that arises is whether or not the listener is perceptually aware of the composer’s procedures: does the listener perceive the intentional movement from key center to key center with no more information than can be grasped by listening?  
  Cuddy & Thompson (1992) used probe tone data from standard pitch hierarchy paradigms to examine the perception of key distance in modulating and non-modulating phrases. They found that participants were able to follow the modulations to a high degree of accuracy, regardless of level of training. In this study, the researchers used simplified settings of Bach chorales, presented in MIDI format. Participants were tasked with measuring how far the pieces traveled relative to the original tonic key. The excerpts modulated no further than two places around the Circle of Fifths (Figure \@ref(fig:Co5)). That is, either not modulating or adding one or two sharps or flats. This approach, in using the reduced chorale settings, is excellent in terms of controlling all of the stimuli, but ignores many of the other musical cues relevant to modulation, such as rhythmic or temporal expectancy [@Narmour2015].  
  Krumhansl & Kessler (1982) modeled perceived tonal organization by having listeners rate probe tones iteratively with each successive addition of a chord in a chord progression. They found that, for closely related keys, whereas the most recent information about the tonic or harmonic area is most influential on a listener’s perception of key, older, more distant information still informs perception. Additionally, they found that the for progressions that modulated to distantly related keys, there was some cognitive lag in adjusting to a new key schema. In selecting their chord progressions, the researchers referenced Arnold Schoenberg’s *Structural Functions of Harmony* (1969), choosing both modulating and non-modulating progressions. The modulating progressions modulated to both close and remote keys, using only pivot chord modulations. One important revelation of this study is the fact that the model developed by Krumhansl & Kessler offers support for many music theoretical precepts put forth by music theorists. Additionally, this study does an excellent job of looking at the phenomenological understanding of harmonic material, but questions remain regarding ecological validity; in that very few pieces of music consist of simple chord progressions with no additional musical features. Hypothetically, there may be musical features that either help or hinder the perception of tonic areas, and the current investigation aims to shed light on those issues.  
  @Toiviainen2003 used listener perceptions and a self-organizing model (SOM) initially developed by @Kohonen1997 to develop a dynamic model of tonality induction. The initial study tested listeners on how strongly a key area was represented at any given moment in a Bach Organ Duetto. They did this by playing the piece twelve times with one of the twelve probe tones superimposed on the audio track. Each one of the 12 versions had a different probe tone, one version for each pitch class. Participants then rated how well each probe tone fit continuously throughout the recording using a slider. Putting these ratings together gave them a tonal hierarchy profile for each instant of the piece. Corellating these instantaneous profiles with the standard profiles for the 24 major and minor keys allwed them to infer the key region in which the listener was hearing the music. They projected these results into a heat-map-like image projected onto a flattened out representation of a toroidal map of key relations (Figure \@ref(fig:Toimgs)). Specifically, it shows that in areas with an unambiguous tonic area, there is a strong association with the location on the toroid related to that tonic. Less related areas show more diffuse levels of association, and modulating sections show associations with both the starting and destination tonic areas. Their results indicate that a listener’s perception of tonic does shift throughout a piece, largely supporting the model developed by @Krumhansl1982a.  
```{r Toimgs, echo = FALSE, out.width='30%', fig.align = 'center', fig.cap=  "The projection of the torus representing key area on a flat plane. The left panel shows the relative positions of the keys in a hypothetical two dimensional space. Imagine this projection with the edges connecting: the top and the bottom connect to form a cylinder and the sides (now the ends of the cylinder) connect to form the torus. The center pane shows a robust or concentrated response pattern, and the rightmost pane shows a diffuse response pattern. (Toiviainen \\& Krumhansl, 2003)",  fig.show='hold'}

knitr::include_graphics(c("images/Toiviainen_B.jpg", "images/Toiviainen_C.jpg", "images/Toiviainen_D.jpg"), )

```
  Raman & Dowling (2017) considered how modulations are perceived in Carnātic (South Indian Classical) music by western and Indian listeners. While western classical music and Carnātic music generally use different surface features and theoretical constructs, the two types of modulations found in Carnātic music, rāgāmālika and grahabēdam, have parallels in western music [@Raman2016]. Rāgāmālika is similar to moving from a given tonic to its parallel minor, where the tonal center remains the same but the mode and pattern of notes around the tonal center change; for example modulating from C major to c minor [@Raman2016]. Grahabēdam is similar to a modulation from one tonal center to another, with a mode change, for example C major to a minor [@Raman2016]. The researchers found that there was an impact of training, but the majority of the difference between groups was between listeners who were enculturated and those who were not. One major difference between western and Carnātic music is that Carnātic music doesn't use harmony in the western sense, where chords are constructed from the notes in the key. Instead, Carnātic music capitalizes on an incredibly complex system of melodies that requires knowledge not only of the notes and intervals in the rāgam, but also certain distinctive melodic features and performance requirements [@Raman2016]. One non-melodic feature of Carnātic music relevant to this study is the sruthi, or continual drone, that consists of tonic, fifth, and octave that sounds through an entire composition. This feature remains constant as the surface features of the music change around it [@Raman2017]. The sruthi acts as a reference pitch, which, although an integral part of the music (as opposed to the probe tone, which is an artificial feature added by an experimenter), allows listeners to compare surface features in a way that doesn’t require the ability to reference pitch on an absolute level. Notably, when modulations occur in Carnātic music, the sruthi remains on the same notes, regardless of which tonic the rāga moves to. For example, if the tonic were to start on C, the sruthi would be C - G - C.^[Carnātic: Sa - Pa - Sa, the western solfege equivalent of Do - Sol - Do] If the tonic then moved to D, as in a grahabēdam, the Sruthi would remain on C - G - C, as opposed to moving with the tonal center to D - A - D [@Raman2017]. This creates an incredibly interesting case study in melodic tension and expectancy that is beyond the scope of the current study.  
  One rarely considered feature of modulations is the intervallic distance between two keys. Where the studies above focus mainly on key correlation, @Kleinsmith2018 looked at how the differences in accuracy between listeners in recognizing melodies transposed between keys that were harmonically distant or intervallicaly distant. Their results showed that pitch distance does affect discriminability, namely that melodies transposed to to intervalically near but harmonically distant (for example C to C\sh) keys were more likely to be recognized than when transposed to harmonically near but intervalically distant keys (for example C to G), and the worst performance was for keys that were both harmonically and intervalically distant (for example C to F\sh). This result converged with the key distance effects found by @Bartlett1980. 

## Critiques of the probe-tone method and other theories of tonality induction
  The probe-tone method is an extremely useful method originally developed by @Krumhansl1979b to measure the goodness of fit of a given tone to complete a scale sequence. It has been repeated numerous times in other contexts, including a more intensive look at the three forms of minor [@Vuvan2011], an investigation into the role of key distance [@Krumhansl1982a], and in an updated model in which the probe tone is played concurrently with the music to ascertain real-time goodness of fit models [@Toiviainen2003a]. The probe-tone method offers a measure of context-dependent pitch correlation and, as described in Krumhansl's 1990 book *Cognitive Foundations of Music Pitch*, a basis for one current theory of tonality induction. The version of this technique demonstrated in @Toiviainen2003a fundamentally uses a framework by which relative consonance is the means by which listeners rate the goodness of fit to a given musical context. This model therefore tracks not the tonal area per se, but rather more acute information about dissonance. Specifically, it tracks the harmonic motion through a given chord progression given an auditory reference point. The tonic area can be inferred from the analysis of listener performance, but it remains that the listener's task measures dissonance given a auditory reference point. Because of what some describe as a functional deficit in this model [@Butler1989; @Butler1990], it is unknown whether or not listeners using Krumhansl's probe-tone method would be able to perceive the shift in tonic area without that auditory reference point.  
  There is, however some converging evidence on this point from Raman and Dowling (2016, 2017). The researchers found similar results in both a modulation detection task similar to the one in the present study and a continuous probe tone task. This still raises the question of reference pitch, due to the characteristic features of Carnātic music described above. These cross-cultural studies also provide evidence that enculturation plays a role in expectancy, namely that westerners seemed apply their schematic knowledge of music to the Carnātic music.  
  Because the probe-tone model depends fundamentally on a listener’s rating of dissonance between a given chromatic pitch and the harmonic and melodic cues at any point during the piece, it doesn’t address a whether or not listeners track key information, and more specifically, information about the relationship between keys. In other words: at any given moment in a musical work, is the tonic area instantiated strongly enough in the listener that they perceive modulations as changes to the tonic area without other external cues? Despite the strengths of the model, there are alternative explanations, including the rare intervals hypothesis [@Butler1989] and Huron and Parncutt's (1993) model that incorporates short-term echoic memory.  
  The rare intervals hypothesis incorporates the distribution of intervals within the diatonic set in its model of tonality induction. In the diatonic set, the perfect fourth appears six times, the major second appears five times, the minor third appears four times, the major third three, the minor second twice, and the tritone once.^[Note that all diatonic intervals are included here by virtue of inversions. The inversion of the perfect fourth is the perfect fifth, the inversion of the minor third is the major sixth, the  inversion of the major third is the minor sixth, the inversion of the major second is the minor seventh, and the inversion of the minor second is the major seventh. The tritone inverted is a tritone.] This theory was championed by @Butler1989, but arose from work by @Browne1980 and @Brown. In the introduction to his chapter in *The Acqusition of Symbolic Skills* @Butler1983 argues that the probe-tone model assumes a priori that the tonic has a position of centrality in the key set, but that the listener would not be aware of this position of centrality. This critique is aimed at the fact that the initial use of the probe-tone method used scale completion as the metric for best fit. This puts the tonic at an automatic advantage because the keys are played ascending, in order of the scale. The initial evidence for the rare interval hypothesis posits that the rarer the interval, the more obvious the cue to the tonic of the key. Further investigation [@Brown1988] suggests that the timing of when the intervals appear throughout a composition also has an effect on tonic identification.  
  The model proposed by @Huron1993 is the only model that incorporates the psychacoustic reality of tonal memory decay with other features, including the rare intervals. It seems obvious to music listeners that "both structural and functional factors" [@Huron1993] contribute to the perception of tonality, but it has been an elusive concept to pin down. The majority of studies cited by @Huron1993 suggest that unattended auditory information is retained for between .5 s and 2 s, with a few outliers suggesting retention lasts for between 2 s and 6 s. Huron argues convincingly that their model works more effectively than does the rare interval hypothesis alone.  
  We suggest that these mechanisms all explain part of the apparatus for tonality induction; it is likely that some sort of parallel processing occurs during music listening. Given the multifaceted nature of music (melody, harmony, rhythm) and its presentation to listeners, it makes sense that listeners would need multiple deductive paradigms to figure out what is "correct". The system likely incorporates all of this information simultaneously to arrive at an answer. A clear example is the pentatonic scale which doesn't have any dissonant intervals, and with only five notes doesn't have the same pattern of intervals between steps, but it exists in the majority of music systems around the world, and doesn't seem like those cultures have any difficulty finding tonic [@Patel2010]. Identifying these mechanisms specifcally is beyond the scope of this investigation, but would make for interesting further study. 


## Modulation as a Music-Theoretical Construct
  Many of our theories about music perception arise from the codification of western music theory during the last 250 years. Many concepts are common to both. As such this section is intended to define some of the music theoretical concepts necessary to understanding modulation, using language from both domains.  
  Fundamental to current models of tonal music perception is the concept of a central or foundational pitch: the tonic [@Krumhansl1990]. This is the note against which other notes and groups of notes are compared to guide our understanding of phrase, tonality, and closure (cadences) [@Sears2015a]. However, as the tonic is not present in every moment of a musical work, our understanding of tonic is also shaped by the harmonic structures that surround it [@Butler1989]. A key or tonal hierarchy is the group of notes that surround the tonic, which are both defined by and, paradoxically, define the tonic itself. The pattern of keys is often presented visually as the circle of Fifths (Figure \@ref(fig:Co5)).
```{r Co5, echo = FALSE, out.width='50%', fig.align = 'center', fig.cap=  "The circle of fifths. Source: Benjamin, Horvit, and Nelson (2003)",  fig.show='hold'}

knitr::include_graphics("images/Co5.jpg")

```
Each letter around the circumfrence of the circle represents the tonic of a major key. Moving clockwise one stop around the circle moves the tonic up by the interval of a fifth and adds a sharp, and moving counter-clockwise around the circle moves the tonic down by the interval of a fifth and adds a flat. Moving from the outer ring to the inner ring moves the tonic to the relative minor, that is, down a minor third to the minor key that shares a key signature and its pitch set with the adjoining major key. As discussed in @Vuvan2011, there are multiple forms of minor (natural, melodic, and harmonic) that have slightly different note sets, but the relative natural minor features the same notes as the major scale.
  Because the tonic of a key is determined both by hierarchy and by the intervallic pattern of the notes that surround it, each key requires sharps (which raise a note by a semitone) or flats (which lower a note by a semitone), and in some cases, such as the harmonic form of minor both, to create that specific pattern. The number of sharps and flats that each key requires are listed in the center of the circle. The key of C has no sharps and no flats because the pattern of sharps and flats aligns itself with the notes in the C major scale. Flats and sharps are always added in the same order; flats in the order B - E - A - D - G - C - F and sharps in the order F - C - G - D - A - E - B.^[Note that each order is the reverse of the other.]   
  These characteristics are part of the reason for the emergence of the rare intervals in a major key, i.e. the tritone between $\hat{7}$ and $\hat{4}$, the minor seconds between $\hat{3}$ and $\hat{4}$ and $\hat{7}$ and $\hat{1}$,^[A note on typography: throughout this paper, letters indicating scale degrees represent both the note itself and the chord based on the letter. Notes on which unaltered triads (chords consisting of 3 notes, each an interval of a third from the one below it) form major chords are denoted by capital letters, and those on which unaltered triads form minor chords are denoted by lowercase letters, and notes on which unaltered triads form diminished chords are indicated by a lowercase letter with a °. The diminished chord built on \sh $\hat{7}$ in minor is not listed, although it is a common alteration. Chords will also be referred to by roman numeral, with major, minor, and diminished being designated in the same way. Hat signs ( $\hat{}$ ) indicate scale degrees, i.e. $\hat{7}$ is the seventh scale degree.] which provide the basis for the rare interval theory [@Browne1980; @Brown1988; @Butler1989] described above.  
  Moving the key up or down a fifth does not require a change in mode, but moving to the relative or parallel minor does.^[However, as @Krumhansl1982a notes, there are conflicting opinions on whether a shift to the relative minor counts as a key change (and therefore a mode change) at all.] Mode change is defined as a change from major to minor mode or vice-versa. The pattern of whole steps (two semitones) and half steps (one semitone) in the minor scale makes it such that, compared to the major scale, the natural minor has a lowered (flatted) $\hat{3}$, $\hat{6}$, and $\hat{7}$. For the relative minor this is accomplished by moving the established key center from the tonic to the submediant, (I - vi, or C major to a minor) and keeping the note set intact.^[In this case it is the notes that remain unchanged while moving the key center down effects the change in the pattern.] Although this is a mode change, the key distance is actually fairly close because of the shared notes between the two keys, and the relatively small distance between the tonic of the two keys. Another common shift is a move to the parallel minor, which is achieved by keeping the same tonic, and lowering $\hat{3}$, $\hat{6}$ and $\hat{7}$. This is illustrated visually in Figure \@ref(fig:keyboards).
```{r keyboards, echo = FALSE, out.width = "50%", fig.align = "center", fig.pos = "!h", fig.cap = "These pictures of a keyboard illustrate the pattern of whole steps and half steps in a C major scale, and in the parallel and relative minor of that scale. Top: The C major scale, as laid out on a piano keyboard. The half steps are indicated with solid brackets above the keyboard and the tritone is indicated with the dashed bracket below the keyboard. Middle: the c natural minor scale, the parallel minor to C Major. Half steps and the tritone are indicated in the same way. Bottom: The a natural minor scale, the relative minor to C major. Note that the half steps for the two minor scales fall between the second and third scale degrees and the fifth and sixth scale degrees, while the tritone falls between the second and sixth scale degrees.", fig.show = "hold"}

   knitr::include_graphics("images/Keyboards.png")

```
  The three types of modulation considered for this study are: direct modulation, common tone modulation, and pivot chord modulation. There are other types of modulation codified in the western classical tradition, but we selected these three to be maximally distinct. Each of these types of modulation features specific characteristic surface features that distinguish it. My hope with this selection is to determine what surface features, if any, are effective cues in listener identification of change in key. 
```{r pcshort, echo = FALSE, out.width='100%', fig.align = 'center', fig.cap=  "Basic example of a pivot chord modulation. The progression is analyzed in C Major and G Major, with the roman numeral analysis in C after the modulation included for illustrative purposes. The highlited chord is the pivot chord.",  fig.show='hold'}
  knitr::include_graphics("images/PCshort.png")

```
  A pivot chord modulation occurs when a composer uses a chord that is common to, and has a similar function^[I.e. pre-dominant function, dominant function. This means that the chord in question serves a certain purpose within the phrase. Tonic is a place of rest, dominant is a place of tension, and pre-dominant sets up the dominant harmony] in, two different keys, serving as a pivot to move from one key to another [@Benjamin2003]. For purposes of this experiment, we selected only excerpts in which the pivot chord was common in both its root (note on which the chord is based) and its quality (major or minor) in the two keys. Figure \@ref(fig:pcshort) illustrates the progression described here. The target key for this type of modulation is often the dominant, and in this case, the vi chord is commonly used as the pivot chord. This creates a vi – II^7^ – V progression in the starting key that matches a ii – V^7^ – I progression in the target key. Note that ii in a major key should be minor, so the II with a seventh added is an altered chord (II^7^), which is made to sound like it fits in the progression by the use of the vi chord, which generally serves pre-dominant function. The example in Figure \@ref(fig:pcexfig)^[All examples transcribed using Finale v. 25 for Windows.] shows a similar progression. The highlited area indicates a I - V - vi^7^ in the key of E\fl, where the vi^7^ serves pre-dominant function in both keys. Interestingly in this example, the highlighted progression in E\fl\ could be interpreted as a deceptive cadence in E\fl, with the closing chord of the cadence (vi^7^, c minor) serving as an elision, extending the phrase and modulating into B\fl, where the c minor chord (now analyzed as ii^7^) serves a pre-dominant function in E\fl. One could argue also that the phrase should be heard entirely in B\fl, with the E\fl\ chord being interpreted as a IV chord in the key of B\fl\  moving to a I^6^ and finally cadencing via ii^7^ - V^7^ - I. However, given the phenomenological nature of music perception supported by the findings of @Krumhansl1982a, the new key area wouldn't be fully instantiated until the completion of the V^7^ - I authentic cadence.
```{r pcexfig, echo = FALSE, out.width='100%', fig.align = 'center', fig.cap=  "Haydn op. 2 No. 3, Mvt. 1, ms. 1 - 27. The large and small highlited areas represent the modulating phrase and the pivot chord, respectively. Source: Haydn (1765/1845), my transcription.",  fig.show='hold'}

knitr::include_graphics("images/pivot_chord.jpg")

``` 
  Though there is a technical distinction between direct and phrase modulation, we consider them functionally equivalent here. Phrase modulation occurs at a phrase boundary: a composer finishes a phrase in one key and begins the next phrase in a new key immediately after, with no transition material. Direct modulation occurs when an abrupt modulation happens somewhere other than a phrase boundary [@Benjamin2003]. These modulations are often made to closely related keys, whether that be the dominant or subdominant, where the tonic relationship is a fifth from the starting key to the target key, or the relative minor, where the tonic moves down minor third, accompanied by a mode change from major to minor. In those cases, there is no more than one note different between the starting and target keys. However, in later eras, composers eschewed these conventions and often used this type of modulation to move much further from the original key.  
  Figure \@ref(fig:dmexfig) is an excerpt from Beethoven's String Quartet in F major, op. 18 No 1, Mvt. 4: Allegro, in which the modulation moves from F major to d minor (tonic to relative minor). The initial I\musFig{6 4} - V - I progression clearly establishes F major before moving back and forth between tonic and dominant harmony. Although there is a C\sh\ in the ninth measure of the excerpt, it's doubtful that a listener would latch onto that as a leading tone to the d minor tonic, for a number of reasons. The passage is moving very quickly and the accidental falls on a weak partial of the sixteenth note grouping on beat two following the tonic of the chord represented in that measure. Also, the surrounding notes outline a dominant harmony in F. Immediately on the downbeat of the eleventh measure, Beethoven switches suddenly to a d minor harmony by landing aggressively on the tonic of the V chord. The scale leading to the tonic, played in unison, uses the a melodic minor scale by raising the B\fl\ to a B\na\ and C\na\ to a C\sh\ and landing on D on the downbeat of the twelfth measure. This dominant-tonic motion and the subsequent a melodic minor scale over the  next two measures solidly establish the d minor harmony. The underscore in the figure indicates the sustained tonic harmony implied by the scale in its entirety.
```{r dmexfig, echo = FALSE, out.width='100%', fig.align = 'center', fig.cap=  "Beethoven String Quartet No. 1 in F Major, op. 18, No. 1, mvt. 4, ms. 17-39. The highlited area represents the modulation from F major to D minor. Although there are two measures highlighted, there is no 'transition material' moving between the two keys, and the modulation occurs directly on the downbeat of measure 11 of the excerpt. Source: Beethoven (1800/1937), my transcription.",  fig.show='hold'}

knitr::include_graphics("images/direct.jpg")

```
  A common tone modulation occurs when a composer uses a single sustained or repeated pitch or dyad (two pitches sounding simultaneously) to link two keys. This can occur either in the middle of a phrase or at a phrase boundary. The common tone is present in both keys but serves a different function in either key. Often, but not necessarily, the pitch is present in the tonic chord of both keys [@Benjamin2003]. Because the only feature linking the two keys is a single pitch or dyad, the two keys need not be closely related. This modulation serves as an efficient way to connect keys that are more distant. Figure \@ref(fig:ctexfig) is an excerpt from Franz Schubert's String Quintet in C Major, D. 956, movement 1. In the sixth measure of the excerpt, the violins both sustain a G across the barline, where the second violin moves down to an E\fl, using the progression $\hat{3}$-\sh$\hat{2}$-$\hat{2}$-$\hat{1}$ in E\fl\ and $\hat{1}$-$\hat{7}$-\fl$\hat{7}$-\fl$\hat{6}$. This containes notes that fit into both keys and allow for a gradual transition to the new tonic. The E\fl\ in the second violin is supported by the second cello on the first beat of the next measure and then repeated by the second cello on the second beat of the measure, firmly establishing the new key.
```{r ctexfig, echo = FALSE, out.width='100%', fig.align = 'center', fig.cap=  "String Quintet in C Major, D. 956, Movement 1, measures 74 - 84. The large and small highlighted areas represent the modulating phrase and the common tone, respectively. The dashed bracket at the top of the excerpt represents the specific area that was played for the participant. Material outside of the bracket included for visual continuity. Note also that, like the score from which it is transcribed, this excerpt contains no key signatures. All altered notes are written into the score. Source: Schubert (1828/1965), our transcription.",  fig.show='hold'}

knitr::include_graphics("images/common_tone.jpg")


```
## Measuring Key Distance
  Traditionally in music, key distance is measured by the distance around the circle of fifths, illustrated in Figure \@ref(fig:Co5). This distance does a good job of measuring distance in terms of note differences, but doesn't accurately reflect the psychophysical or perceptual difference between keys, because, for example, a modulation from major to relative minor and major to its dominant would have the same distance, which does not account for the change in mode from major to minor, the added sharp note in the dominant key signature, or the effect of the intervallic distance between the tonics of two keys [@Kleinsmith2018].  
  In an effort to most effectively map key distance, we looked to Krumhansl's (1990) work that also served as the basis for @Toiviainen2003a. Krumhansl used multi-dimensional scaling to map each of the keys, using their correlation profiles, [@Krumhansl1982a; @Krumhansl1990] onto a four-dimensional space. These were then mapped onto a torus, which was used by @Toiviainen2003a as the spatial mapping for their concurrent probe-tone visualization. we used this table as a representation of a four-dimensional vector space and calculated the euclidean distance between each set of four coordinates. This method helped me to arrive at a distance between each of the keys that accurately reflected the key correlation (i.e. the distance around the circle of fifths) and the psychophysical effects of mode change. Because each key is located at a specific point in four dimensional space, each of the calculated distances between the keys is unique. However, there were some patterns that emerged. For example, the calculated distance between two keys separated by the interval a fifth, i.e. a tonic and its dominant or subdominant, or one step in either direction around the circle of fifths (C - G or C - F) was approximately 0.86. This makes sense both in terms of the key distance and the intervallic relationship between the tonics of the two keys. The smallest distance between any two keys was between a given tonic and its relative minor (e.g. C to a minor), approximately 0.65. The largest distance between any two keys was approximately 2, which was the distance between any two keys separated by a tritone (e.g. I - \fl V/ \sh IV; C to F\sh / G\fl). The largest key distance for any excerpt in this experiment was 1.892, between c minor and A major. An ANOVA run on the key distances of the stimuli indicated that there was not a significant difference in key distances between modulation types. 



## Present questions

  The questions we consider for the present experiment are as follows: Do music listeners passively retain information on key region independent of topical, salient features of the music? To what extent does training affect the storage, processing, and access to that information, if it exists? What topical features influence our understanding of key regions and the movement between them? What is the balance between melodic and harmonic features contributing to that understanding?  
  To be clear, this investigation does not claim that these musical elements are truly independent. We seek rather to investigate the level to which they are considered independently, or the level to which each is effective when making judgments within a musical context. For a long time, studies surrounding key area processing focus solely on either melodic material [@Dowling1986; @Bartlett1980; @Krumhansl1982c] or harmonic material [@Thompson1989a]. Only recently have researchers begun to use more naturalistic stimuli, such as excerpts or midi recreations in their research [@Toiviainen2003a]. @Krumhansl1983 argues convincingly that the various features of a musical piece are all interdependent and contribute to the processing of key area. It remains a question, however, what musical information takes priority in terms of salience during listening, and whether or not that information receives similar priority during processing. Both melody and harmony are technically topical, salient features, although we would guess that casual music listeners tend to follow melody more than harmony, unless the composition specifically guides the listener's attention to the harmonic material. The information that would be "background" is not the harmonic motion per se, or even the chord qualities, but rather the relative implications, or functions of the chords in the context of the key. These chordal functions are readily understood upon harmonic analysis of a written score, but may not be understood by passive or even active listening. 

# Methods  
## Participants and group assignments  
```{r participant numbers, include = FALSE}

meanage <-  mean(subjectdata$Age)
sdage   <-  sd(subjectdata$Age)
byc     <-  subjectdata[order(subjectdata$Class), ]
ltmean  <-  mean(byc$Years.of.formal.training[1:60])
ltsd    <-  sd(byc$Years.of.formal.training[1:60])
mtmean  <-  mean(byc$Years.of.formal.training[61:120])
mtsd    <-  sd(byc$Years.of.formal.training[61:120])
htmean  <-  mean(byc$Years.of.formal.training[121:180])
htsd    <-  sd(byc$Years.of.formal.training[121:180])

```
  The majority of participants for this study were adults selected from the UT Dallas undergraduate SONA pool. These students were compensated with credit towards their psychology research exposure requirements. Some participants were also recruited from the music department at Northwestern State University in Natchitoches, Louisiana (NSULA), and some professional musicians and music educators from around the region were recruited through direct personal correspondence. Participants who were not students at UT Dallas were not compensated.  
  Approximately equal numbers of male and female participants participated in the study (M = 92, F = 87, NB^[Non-Binary] = 1), but gender was not considered. Participants mean age was `r printnum(meanage)`,  (*SD* = `r printnum(sdage)`).  Participants were excluded if they met any or all of the following criteria: exposure to or training in South Indian Classical (Carnātic) Music; absolute pitch; or a hearing disability such as deafness, tinnitus, or amusia. Participants were evenly divided into three groups (n = 60) based on years of formal music training. Participants with zero to two years of formal music training were assigned to the nonmusician/untrained category, those with three to 10 years of formal training were assigned to the moderately-trained category, and those with 10 years or more of training were assigned to the highly-trained musician category. The nonmusician group had a mean of `r printnum(ltmean)` years of training (*SD* = `r printnum(ltsd)`), moderately-trained musicians had a mean of `r printnum(mtmean)` years of training (*SD* = `r printnum(mtsd)`), and highly trained musicians had a mean of `r printnum(htmean)` years of training (*SD* = `r printnum(htsd)`). Additionally, participants who had less than 10 years of training but had completed an Advanced Placement^TM^ or college-level aural skills training course also qualified for the highly-trained musician category. There were only four participants who met those criteria, two of whom had 9 years of formal music training, one who had 8, and one who had 5. For purposes of this study, formal music training was considered any time spent pursuing music in a formal setting, including large ensemble experience, small ensemble experience, and private lessons.  

## Stimuli  

  Stimuli were selected from the string quartet, quintet, and sextet works of Joseph Haydn, Roman Hofstetter, Wolfgang Amadeus Mozart, Ludwig van Beethoven, Franz Schubert, and Johannes Brahms. The string quartet idiom was selected to most effectively control the effects of timbre.  The earliest date of composition of any of the excerpts used was 1762 (Haydn Op. 1, No. 1) and the latest date of composition was 1890 (Brahms Op. 111, No. 2). A full list of excerpts and recordings is included in Appendix 1. Each of the excerpts were chosen to meet specific structural criteria. These criteria included length, the presence of a single modulation, surrounded by a stable tonic area on either side, and enough time to instantiate the intial key before the modulation occurred. The shortest stimulus was 21.05 s and the longest was 59.66 s, the mean length was 28.98 s. Each stimulus included at least six seconds of stable establishment of tonic before the modulation occurred.  
  Each excerpt was ripped from its source CD using fre:ac version 1.0.32 [@Kausch2018], an open-source audio converter. Stimuli were presented as .wav files to ensure the highest quality audio signal. Stimuli were presented either using Koss model UR 20 headphones, if participants were run in the lab, or on Vic Firth^TM^ brand over-ear isolation headphones, to ensure presentation quality and isolation from external noise. Because stimuli were authentic recordings, they were presented without volume edits to preserve musicality. Participants adjusted volume to their comfort.  
  Trial stimuli were differentiated into three groups (n = 14), with each group representing a specific type of modulation: direct, pivot chord, and common tone. These three types of modulations were chosen to be maximally distinct. A fourth group of stimuli presented were lures that did not modulate (n = 7). The non-modulating stimuli were selected such that one excerpt from each composer was represented, except for Haydn, who was represented twice. Thus there were a total of 49 excerpts presented to participants, with a combined duration of 22m 59s.   
  The test stimuli were balanced across types of modulation, with 14 of each type. The test stimuli were also balanced across the three modulating conditions as to how many of each type changed mode during the modulation, as this was hypothesized to serve an obvious cue that a modulation has occurred. The excerpts were not balanced by starting mode, however, and more common tone excerpts started in the minor mode than any other. This is likely a negligible artifact, however, because the minor mode is not in an of itself a cue that a modulation is going to happen. Tempos for all stimuli were assessed to determine approximate tempo, and to rule out any individual effects of tempo a simple regression predicting *A*' from tempo was nonsignificant. It is worth noting that for each of the excerpts, the metronome marking was measured according to the apparent pulse, not necessarily according to the meter. For example, if an excerpt in triple time, i.e. a 3/4 time signature, were extremely fast, the apparent pulse may fall on the dotted half note instead of the quarter note. Across all 14 stimuli, for each modulating condition, the average tempo was as follows: pivot chord, 112.64 beats per minute (bpm); direct, 115.00 bpm, and common tone, 114.50 bpm. Across the range of tempi included in the test stimuli, from a minimum of 43 bpm to a maximum of 236 bpm, the overall average was 114.05.  
  For each stimulus, we determined through aural and theoretical score analysis a critical time at which the modulation or lure imitation occurred. Using that as a reference, we selected a time window in which a response would indicate an accurate reaction to, and therefore, perception of, of the modulation. The window for each modulation began either on the modulation, as in the case of a direct modulation, or at the first indication of a modulation, which for the common tone excerpts was the beginning of the sustained note. The window for each modulation ended at the confirmation of the new key. For most excerpts this occured on a tonic chord of an authentic cadence in the new key. Most stimuli also had a designated false alarm window, in which a response would count as a false alarm. These false alarm windows included such artifacts as secondary dominants^[For example: V/V - V - I, read as "five of five going to five, going to one". The "five of five" is the secondary dominant here - the dominant of the dominant.], strongly emphasized minor chords, or non-harmonic tones, all of which resolved to the original tonic. None of these false alarm windows overlapped with the modulation windows and none could be considered true modulations.   
  
## Procedure
Participants participated in the experiment at the MPaC lab at UT Dallas Main Campus, or if participants were unable to travel to UTD (as in the case of the participants from NSULA), we made arrangements to run the participants in a quiet, distraction free environment, either a quiet room in the participant’s home or an unused classroom on NSULA’s campus. Following consent procedures, participants completed a questionnaire about the extent of their music training. Researchers then gave participants instructions on how to complete the task. Participants who were unfamiliar with the concept of a modulation received a brief introduction to the concept of tonality and modulation. Many participants who were not trained musicians were more comfortable with the term “key change”, and we used that connection to help those participants understand the overall concept. Once participants expressed a satisfactory understanding of the concepts, they were given instructions as to how to complete the task. Participants were then given a brief explanation of what they were to listen for, and that they should press the designated key on the keyboard when they hear the modulation within the stimulus. Participants were informed that they could respond as many times as they liked during any given excerpt, but that each excerpt only contained at most one modulation, and there were some excerpts that did not modulate. Participants were not informed in advance of what types of modulation to expect.^[A few highly trained musicians asked if they should consider the relative minor a key change. The response given was always "Respond when you think you're in a new key"] Stimuli were presented using Matlab version R2009B [@Various2009] using code adapted from @Raman2017. Responses in the modulation window were recorded as hits and responses in the false alarm window were recorded as false alarms. Responses outside of either window were evaluated as noise and not considered for the purposes of this analysis. Participants moved through the excerpts at their own pace, beginning each excerpt at their leisure following the completion of the previous one. Participants were allowed to take breaks as they felt necessary. Excerpts were presented in a different random order for each subject to mitigate any effects of ordering.

## Design & Hypotheses  
The planned analyses we considered for this experiment dealt with training, type of modulation, key distance, and response time. The first analysis we performed was to find *A*' given hits and false alarms for all participants for all excerpts.^[*A*' is an estimate of the unbiased proportion of correct responses where chance equals 0.50.] we then calculated average response time for each participant, and average time for each participant by modulation type, all of which were compared using planned Tukey tests with corrections for multiple comparisons.  
Hypothesis one was that groups of participants who have greater levels of training, across all modulation types, would be more accurate. Should this hypothesis be supported, it would represent evidence that training is necessary for key retention, and that it is not a passive process. Should it not be supported, and we find instead that accuracy is independent of training, it would suggest that key retention is passive and training is not necessary.  
Hypothesis two was that, across levels of training, the responses to the pivot chord modulation would be the least accurate, responses to the direct modulations would be the most accurate, and the responses to common tone modulations would fall in the middle. This hypothesis most directly addresses the question of topical salient musical features, and makes two specific predictions. Firstly, that different types of modulations, and the topical features of each, will inform listener perceptions to different degrees, and that the movement between the two keys will be recognized with different levels of accuracy. Secondly, it makes a specific prediction about the relative levels of accuracy that each of the specific modulation types will see. If participants were to respond to all of the modulation types with equal accuracy, it would suggest that topical features and key area recognition are independent of one another. This would support the idea that the perception of movement between key areas, and therefore the recognition of tonic area per se, are unaffected by topical features.  
Hypothesis three was that key distance and mode change would be more accurate predictors of modulation perception, with excerpts that feature a mode change as part of the modulation being related to more accurate responses, and greater key distance also being related to more accurate responses. This hypothesis, like hypothesis number two, also deals with topical features. Should thse features be found to represent more accurate responses, it would suggest that those specific features guide perception to a greater extent than other types of musical features.  
Hypothesis four was that trained listeners would respond faster to the modulations than untrained listeners. Hypothesis four, like question one, addresses the question of training, but from a different perspective. Faster response time in trained listeners would suggest that the key area processing is more efficient as a result of training.   

# Results 
## Training Level and Modulation type
``` {r results, include = FALSE}
library(MBESS)
library(PTCA4CATA)
options(tinytex.verbose = TRUE)
hyp1 <- apa_print(m.c.aov) # 3 X 3 Mixed ANOVA of A' given Modulation type (within) and training level (between)

h1effects <- data.frame(matrix(nrow = 6, ncol = 4))

rownames(h1effects) <- c("Nonmusicians vs. Moderately-trained", "Highly-trained vs. Nonmusicians", "Highly-trained vs. Moderately-trained", "Direct Modulation vs. Pivot Chord Modulation", "Common Tone Modulation vs. Pivot Chord Modulation", "Common Tone Modulation vs. Direct Modulation") 
colnames(h1effects) <- c("*d*", "lower limit", "upper limit", "*p* value")
h1effects[1:3, ] <- tukeybyall$class
h1effects[4:6, ] <- tukeybyall$modtype 
h1effects[ , 4] <- printp(h1effects[ ,4])
aprstandard <- sd(aprimelong[181:720, 4])
h1effects[ ,1] <- (h1effects[ ,1]/aprstandard)

h1cod <- matrix(nrow = 6, ncol = 3)

for (i in 1:dim(h1effects[1]))
{j <- 180

fart <- ci.smd(smd = h1effects[i,1], n.1 = j, n.2 = j, alpha.lower = .975, alpha.upper = .975)
h1cod[i,1] <- fart$smd
h1cod[i,3] <- fart$Lower.Conf.Limit.smd
h1cod[i,2] <- fart$Upper.Conf.Limit.smd

   }

h1effects[ ,1:3] <- h1cod


h1exp <- kable(h1effects, format = "latex", booktabs = TRUE, linesep = "", 
               digits=2, align = "c", col.names = c("$d$", "lower limit", "upper limit", "$p$ value"), escape = F) %>%
            kable_styling(latex_options = c("HOLD_position", "scale_down")) %>%
            footnote(number = c("Significance values adjusted for a familywise error rate of 95%; ", '"lower limit" and "upper limit" refer to the lower and upper limits of the 95% confidence interval for effect size.'))

hyp2   <- apa_print.lm(Aprbykey) # simple regression of A' by key distance
hyp2.1 <- apa_print.lm(Aprbykey.mcdc) # multiple regression of A' by key distance * mode change
hyp2.2 <- apa_print.lm(Aprbykey.mcrc) # same as hyp2.1, with mode Reverse-coded, so that the mode change = 0 and no mode change = 1
hyp3   <- apa_print(subrt.aov) # ANOVA of reaction time by training level
hyp3.1 <- apa_print.lm(stimrt.lm)

h3effects <- data.frame(matrix(nrow = 6, ncol = 4))
rownames(h3effects) <- c("Nonmusicians vs. Moderately-trained", "Highly-trained vs. Nonmusicians", "Highly-trained vs. Moderately-trained", "Direct Modulation vs. Pivot Chord Modulation", "Common Tone Modulation vs. Pivot Chord Modulation", "Common Tone Modulation vs. Direct Modulation") 
colnames(h3effects) <- c("*d*", "lower limit", "upper limit", "*p* value")
h3effects[1:3, ] <- subrt.tukey$`factor(class)`
h3effects[4:6, ] <- stimrt.tukey$`factor(type)`
h3effects[ , 4] <- printp(h3effects[ ,4])
rtstandard <- sd(rtlong$Reaction_Time, na.rm = T)
h3effects[ ,1] <- (h3effects[ ,1]/rtstandard)
h3cod <- matrix(nrow = 6, ncol = 3)

for (i in 1:3)
{j <- 180

fart <- ci.smd(smd = h3effects[i,1], n.1 = j, n.2 = j, alpha.lower = .975, alpha.upper = .975)
h3cod[i,1] <- fart$smd
h3cod[i,3] <- fart$Lower.Conf.Limit.smd
h3cod[i,2] <- fart$Upper.Conf.Limit.smd

}

for (i in 4:6)
{j <- 14

fart <- ci.smd(smd = h3effects[i,1], n.1 = j, n.2 = j, alpha.lower = .975, alpha.upper = .975)
h3cod[i,1] <- fart$smd
h3cod[i,3] <- fart$Lower.Conf.Limit.smd
h3cod[i,2] <- fart$Upper.Conf.Limit.smd

}


h3effects[ ,1:3] <- h3cod


hthreeexp <- kable(h3effects, format = "latex", booktabs = TRUE, linesep = "", 
               digits=2, align = "c", col.names = c("$d$", "lower limit", "upper limit", "$p$ value"), escape = F) %>%
            kable_styling(latex_options = c("HOLD_position", "scale_down"))  %>%
           footnote(number = c("Significance values adjusted for a familywise error rate of 95%.", '"lower limit" and "upper limit" refer to the lower and upper limits of the 95% confidence interval for effect size.'))

barf <- data.frame(tukeybyall$`class:modtype`)
tag <- barf[which(barf$'p.adj' < .05), ]
tag <- tag[c(2, 16, 5, 8, 11, 14), ]
tag[ ,1:3] <- tag[ ,1:3]/aprstandard
tag[ ,4] <- printp(tag[ ,4])
for (i in 1:dim(tag[1]))
{j <- 180
fart <- ci.smd(smd = tag[i,1], n.1 = j, n.2 = j, alpha.lower = .975, alpha.upper = .975)
tag[i,1] <- fart$smd
tag[i,3] <- fart$Lower.Conf.Limit.smd
tag[i,2] <- fart$Upper.Conf.Limit.smd
}
tag[ ,1:3] <- round(tag[ ,1:3], 2)
rownames(tag) <- c("1 - CT vs. 1 - PC", "1 - CT vs. 1 - DM", "2 - DM vs. 2 - PC", "2 - CT vs. 2 - PC", "3 - DM vs. 3 - PC", "3 - CT vs. 3 - PC")

cap <- data.frame(c("Groups: 1 = Untrained, 2 = Moderate training, 3 = Highly trained; Modulation types: CT = Common Tone, D = Direct,",
                         "PC = Pivot Chord; Significance values after adjusting for multiple comparisons, lower limit and upper limit refer to the",
                         "lower and upper limits of the 95% confidence interval for effect size."))

tagtab <- kable(tag, format = "latex", booktabs = TRUE, linesep = "",
                digits=2, align = "c", col.names = c("Cohen's $d$", "lower limit", "upper limit", "$p$ value"), escape = F, 
                caption = "Significant Differences Among Training Levels and Modulation Types") %>%
                kable_styling(full_width = F, font_size = 10, latex_options = "scale_down")  %>%
                column_spec(column = c(2:5), width = "1.2in") %>%
                footnote(cap[,1])

                            

cbmtints <- kable(tag)
cbmtints

taggedtable <- apa_table(tag, caption = "Significant Differences Among Training Levels and Modulation Types", note = 'Groups: 1 = Untrained, 2 = Moderate training, 3 = Highly trained. \\nModulation types: CT = Common Tone, D = Direct, PC = Pivot Chord. \\nSignificance values adjusted for a familywise error rate of 95%. \\n"lower limit" and "upper limit" refer to the lower and upper limits of the 95% confidence interval for effect size.')


ofix <- aprimelong$modtype == "apr_ovr"
pfix <- aprimelong$modtype == "apr_pc"
dfix <- aprimelong$modtype == "apr_dm"
cfix <- aprimelong$modtype == "apr_ct"

aprimelong$modtype <- as.character(aprimelong$modtype)

aprimelong$modtype[ofix] <- "1. Overall"
aprimelong$modtype[pfix] <- "2. Pivot Chord"
aprimelong$modtype[dfix] <- "3. Direct"
aprimelong$modtype[cfix] <- "4. Common Tone"

aprimelong$modtype <- as.factor(aprimelong$modtype)

aprimesummary <- ddply(aprimelong, c("class", "modtype"), summarise, 
                       N = length(aprime),
                       mean = mean(aprime),
                       sd = sd(aprime),
                       se = sd/sqrt(N))

ameans <- ddply(aprimelong, "modtype", summarise,
                N = length(aprime),
                mean = mean(aprime),
                sd = sd(aprime),
                se = sd/sqrt(N))

apm.g <- ggplot(ameans, aes(x = modtype, y = mean,  color =  modtype, shape = modtype)) +
                  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, position = pd, size = 1) +
                  #geom_line(size = .5,  position = pd) +
                  geom_point(position = pd, size = 4, fill = "white") +
                  geom_text(aes(x = modtype, y = mean, label=round(mean, 2), hjust = 1.5)) +
                  xlab("Modulation Type") +
                  ylab("A`") +
                  scale_colour_hue(name="Modulation Type",
                                   breaks = c("1", "2", "3"),
                                   labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                                   l=40) +
                    ggtitle("Differences in means between modulation conditions, across training levels") +
                    expand_limits(y = c(.5:1.1)) +
                    scale_y_continuous(breaks = c(.5, .6, .7, .8, .9), labels = c(.5, .6, .7, .8, .9)) +
                    theme_bw() +
                    theme(legend.position = "none")

apm.g_apa_bw <- ggplot(ameans, aes(x = modtype, y = mean,  fill = modtype)) +
   geom_bar(color = "black", stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, size = 1, position = position_dodge(.9)) +
   geom_text(aes(x = modtype, y = mean, label=round(mean, 2)), position = position_dodge(.9), vjust = -4, color = "black", size = 3) + 
   scale_fill_grey(guide = FALSE) +
   theme_apa() +
   coord_cartesian(ylim=c(.5, .9)) +
   xlab("Modulation Type") +
   ylab("Mean A'")

apm.g_apa <- ggplot(ameans, aes(x = modtype, y = mean,  fill = modtype)) +
   geom_bar(color = "black", stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, size = 1, position = position_dodge(.9)) +
   geom_text(aes(x = modtype, y = mean, label=round(mean, 2)), position = position_dodge(.9), vjust = -2, color = "black", size = 4) + 
   scale_fill_discrete(guide = FALSE, l=50) +
   theme_apa() + 
   coord_cartesian(ylim=c(.5, .9)) +
   xlab("Modulation Type") +
   ylab("Mean A'")



apm.l <- ggplot(aprimesummary, aes(x = modtype, y = mean, color = class, shape = class)) +
                  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, position = pd, size = 1) +
                  geom_line(aes(group = class, color = class), size = .5,  position = pd) +
                  geom_point(position = pd, size = 4, fill = "white") +
                  geom_text(aes(x = modtype, y = mean, label=round(mean, 2), hjust = 1.8)) +
                  xlab("Modulation Type") +
                  ylab("A`") +
                  scale_shape_discrete(labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)")) +
                  scale_color_discrete(labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)")) +
                  labs(shape = "Level of Training", color = "Level of Training")+
                  scale_color_hue(breaks = c("1", "2", "3"),
                                   labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                                   l=40, 
                                   ) +
                    ggtitle("The Effect of Training on Modulation Perception") +
                    expand_limits(y = c(.5:1.1)) +
                    scale_y_continuous(breaks = c(.5, .6, .7, .8, .9), labels = c(.5, .6, .7, .8, .9)) +
                    theme_bw() +
                    theme(legend.justification = c(1,0), legend.position = c(1,0.01))
                  
apm.b_apa_bw <- ggplot(aprimesummary, aes(x = modtype, y = mean, fill = class)) +
   geom_bar(color = "black", stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, size = 1, position = position_dodge(.9)) +
   geom_text(aes(x = modtype, y = mean, label=round(mean, 2)), position = position_dodge(.9), vjust = -4, color = "black", size = 2.5) + 
   scale_fill_grey(labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                        breaks = c("1", "2", "3"), ) +
      scale_x_discrete(breaks = c("1. Overall", "2. Pivot Chord", "3. Direct", "4. Common Tone"), 
                    labels = c("Overall \nmean", "Pivot \nChord", "Direct", "Common \ntone")) +
   theme_apa() + 
   coord_cartesian(ylim=c(.5, .9)) +
   xlab("Modulation Type") +
   ylab("Mean A'")


apm.b_apa <- ggplot(aprimesummary, aes(x = modtype, y = mean, fill = class)) +
   geom_bar(color = "black", stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = .1, size = 1, position = position_dodge(.9)) +
   geom_text(aes(x = modtype, y = mean, label=round(mean, 2)), position = position_dodge(.9), vjust = -4, color = "black", size = 2.5) + 
   scale_fill_discrete(labels = c("Nonmusician (< 2 yrs)", "Moderate training (2 - 10 yrs)", "Highly Trained (10+ yrs)"), 
                        breaks = c("1", "2", "3"), l=50) +
   scale_x_discrete(breaks = c("1. Overall", "2. Pivot Chord", "3. Direct", "4. Common Tone"), 
                    labels = c("Overall \nmean", "Pivot \nChord", "Direct", "Common \ntone")) +
   theme_apa() + 
   coord_cartesian(ylim=c(.5, .9)) +
   xlab("Modulation Type") +
   ylab("Mean A'")      



```

  A three by three mixed anova comparing modulation type and training level with *A*' as the dependent variable indicated significant simple main effects of training, `r hyp1$statistic$class[1]`, and modulation type, `r hyp1$statistic$modtype[1]`, as well as a significant interaction, `r hyp1$statistic$class_modtype[1]`. A Tukey test indicated that there was a significant difference between the highly trained group and the nonmusician group, Cohen's *d*^[Hereafter listed simply as *d*, not to be confused with the signal detection theory measure *d*'. In the current context, *d* represents the standardized mean difference between the means of the groups, calculated using the pooled standard deviation as a standardizer.] = `r round(h1effects[2, 1], 2)`, 95% CI [`r round(h1effects[2, 2], 2)`, `r round(h1effects[2, 3], 2)`], *p *`r h1effects[2, 4]`, as well as between the highly trained group and the moderately-trained group, *d =* `r round(h1effects[3, 1], 2)`, 95% CI [`r round(h1effects[3, 2], 2)`, `r round(h1effects[3, 3], 2)`], *p* < .001. However, the difference between the moderately-trained group and the nonmusician group was not significant, *d* = `r round(h1effects[1, 1], 2)`, 95% CI [`r round(h1effects[1, 2], 2)`, `r round(h1effects[1, 3], 2)`], *p* =  `r h1effects[1, 4]`. These results are all depicted graphically in  Figure \@ref(fig:grapha).   
  The simple main effect of modulation type was significant, and a Tukey test indicated that the differences between each of the modulation types were all significant. Between direct modulation and pivot chord modulation was a difference of *d* = `r round(h1effects[4, 1], 2)`, 95% CI [`r round(h1effects[4, 2], 2)`, `r round(h1effects[4, 3], 2)`], *p* `r h1effects[4, 4]`, between common tone modulation and pivot chord modulation was a difference of *d* = `r round(h1effects[5, 1], 2)`, 95% CI [`r round(h1effects[5, 2], 2)`, `r round(h1effects[5, 3], 2)`], *p* `r h1effects[5, 4]`, between common tone modulation and direct modulation was a difference of *d* = `r round(h1effects[6, 1], 2)`, 95% CI [`r round(h1effects[6, 2], 2)`, `r round(h1effects[6, 3], 2)`], *p* = `r h1effects[6, 4]`.  The results of the analysis by modulation type are all depicted in Figure \@ref(fig:graphb), and within-group significant differences by modulation type are presented in Table 1. 
```{r tagtable, echo = F}

tagtab

```
```{r grapha, echo = F, out.width='100%', fig.align = 'center', fig.cap=  "Relative accuracy, represented by A', of particpants on the modulation perception task. Error bars represent the standard error of the mean. ", fig.show = 'hold'} 

apm.b_apa_bw
#apm.b_apa

```
```{r graphb, echo = F, out.width='100%', fig.align = 'center', fig.cap=  "Relative accuracy, represented by A', of particpants on the modulation perception task, overall and modulation condition means. Error bars represent the standard error of the mean.", fig.show = 'hold'} 

apm.g_apa_bw
#apm.g_apa

```  
```{r mcgraph, echo = F, out.width = '100%', fig.align = 'center', fig.cap=  "Multiple regression using key distance and mode change as predictors.",  fig.show='hold'}
  
abkmcecp_bw

```
## Key Distance
  A simple regression predicting *A*' from key distance was significant, `r hyp2$full_result$modelfit$r2`, `r hyp2$estimate$modelfit$r2_adj`, such that greater key distance indicated less accurate response, `r hyp2$estimate$Key_dist`. A multiple regression including both key distance and mode change was also found to be significant overall, `r hyp2.1$full_result$modelfit$r2`. In this model, key distance was not found to be significant, `r hyp2.1$full_result$Key_dist`, but mode change was marginally significant, `r hyp2.1$full_result$mode_change`, while the interaction between the two was found to be significant `r hyp2.1$full_result$Key_dist_mode_change`.  These results are illustrated in Figure \@ref(fig:mcgraph).  

## Reaction Time
  Analysis of reaction time across training levels indicated a significant effect of training level, `r hyp3$full_result$factorclass`, and a Tukey test revealed significant differences between the highly trained musicians and non musicians group, *d* = `r round(h3effects[2, 1], 2)`, 95% CI [`r round(h3effects[2, 2], 2)`, `r round(h3effects[2, 3], 2)`], *p *`r h3effects[2, 4]`, and  the highly trained group and the moderately-trained group, *d* = `r round(h3effects[3, 1], 2)`, 95% CI [`r round(h3effects[3, 2], 2)`, `r round(h3effects[3, 3], 2)`], *p* = `r h3effects[3, 4]`, such that the highly trained musicians reacted more slowly to the modulations. However, the difference between the moderately-trained group and the nonmusican group was not significant, *d* = `r round(h3effects[1, 1],2)`, 95% CI [`r round(h3effects[1, 2], 2)`, `r round(h3effects[1, 3], 2)`], *p* = `r h3effects[1, 4]`. There was no difference in reaction time between modulation types, all *p* > .05. Finally, a regression predicting reaction time from key distance was not significant, `r hyp3.1$full_result$KeyDistance`. The results of reaction time by training are presented in Figure \@ref(fig:rtgraph).
```{r rtgraph, echo = F, out.width = '100%', fig.align = 'center', fig.cap = "Graph representing the means the reaction times of the training groups. Error bars represent the standard error of the mean."}
  
group.rt.apa

```
# Discussion  
  In many respects, the results of the present study offer support for the existing hypotheses surrounding key distance and training, and in others it offers contradicting evidence. It also offers an interesting perspective on certain topical features that may help us understand what guides our understanding of tonic and tonic regions.  
   The significant results on response accuracy confirm a fairly common-sense idea that training helps a listener more accurately discern if and when a modulation occurs. However, even the untrained listeners performed at above-chance levels on the task overall, suggesting that in general, listeners who are familiar with western music and its harmonic language are able to identify more often than not when a modulation occurs. This suggests that at some level, information regarding key area is encoded even in the minds of untrained listeners. Looking at the overall results of training, it's likely that the quality of training is equally important as the amount. The majority of highly-trained listeners were in some way professional musicians, or training to be professional musicians; they are people for whom accuarate aural perception is a professional necessity. It makes sense, then, that they would perform better on aural-skills tasks than untrained or moderately-trained listeners. It's also interesting that moderately-trained listeners, who have up to 10 years of formal training, are significantly less accurate than the highly trained listeners. This, of course, raises the question: are professional musicians better at aural skills tasks because they are professional musicians, or are they professional musicians because they are better at aural skills tasks?  
  With regard to the result of the modulation types, perhaps the most interesting result is how poorly the highly trained listeners performed in the pivot-chord condition. It was by far the worst condition among the highly trained musicians and, although the differences were not significant, they had the lowest mean score among all testing groups. This phenomenon makes sense in light of what many of the participants said in their debriefing, namely that they weren't sure if many of the stimuli were true pivot chord modulations or if they were secondary dominants, and once they were sure, it was too late; either the excerpt had ended or the window had passed (of which they would not have been aware). The interference is likely to have come from the instructions. Participants were informed that there would be trials that did not modulate, and one of the most prominent cues in the surface features of pivot chord modulations is also a primary cue for a temporary tonicization, to which trained listeners would have been sensitive and untrained listeners would not. This feature is the secondary dominant (the V/V) described above. A temporary tonicization is therefore very similar to a pivot chord modulation in terms of both melodic features (altered or out of key notes functioning as leading tones) and harmonic features (altered or out of key chords functioning as secondary dominants), the primary difference between the two being whether or not the excerpt remains in the new key or if it returns to the original starting key. As stated above, all of the modulating excerpts included in this study remain in the target key, but the fact that the excerpts were short, may also have been a contributing factor: although the new keys were confirmed by an authentic cadence in the new key, it's likely that the highly-trained participants intentionally avoided responding until they were sure that the excerpt wasn't going to modulate back, but by the time they were sure that it wasn't going to modulate back, the excerpt had ended.  
  Pivot chord modulations led to the lowest *A*' score across all types of modulation. This also makes sense in terms of the harmonic features of the pivot chord modulation. Incorporating the rare intervals theory described above[@Butler1989], in any given key, the tritone between $\hat{7}$ and $\hat{4}$ is the most reliable predictor of a tonic region. Altering a note in the key seems to have the perceptual effect not of replacing that note, but expanding the set of notes in the perceptual window to include the chromatic alteration, at least until the intertia of perceptual experience in the piece erases the original note from the framework. None of these things happen in a purely melodic context, and incorporating the harmonic context gives us a clearer picture of this process.  In general, the goal of composotion in this idiom is smoothness, and the way to achieve that smoothness is by preceeding the altered note by a chord that assists in making that note (and therefore the chord to which it belongs) appear "correct" (Figure \@ref(fig:pcshort)). Thus, by expanding the tonic area to include the notes necessary to tonicize the new key, the composer enables the cadence in the new key to be effectively incorporated into the scheme without any perceptual jarring. However, those who are familiar with these cues will still recognize them for what they are. @Dowling1986 provides evidence that highly trained listeners encode scale steps explicitly when they're listening to music, and this reflects that idea: the trained listeners were able to recognize the altered scale tones not only for their rank in the key set, but also for their function.  Given the precision required for this task and subtlety of this compositional technique, it is very interesting that the untrained listeners performed as well as they did. Further analysis here would be warranted to investigate what other factors play into this result.  
    The greater spread between groups on the direct modulation more accurately exposes the effect of training. Whereas untrained listeners performed approximately as well on the direct modulations as the pivot chord modulations, both of the other groups performed significantly better on direct modulations than pivot chord modulations. Given the spread, it is likely that this effect is almost entirely dependent on training and has very little to do with surface features. People with music training of any kind are more familiar with the concepts of key and modulation, and are therefore consistently more accurate when responding, whereas those who are untrained are relying on whatever system seems to be tracking the tonic region.   
    For common tone modulations, all three training groups were clustered around *A*' = .8, which is by far the best performance overall. This result also supports the existing theories on pitch region perception and the surface features of the common tone modulations. These surface features align most closely with those of the probe-tone test paradigm, where the sustained or repeated note takes the place of the probe tone and serves as a reference pitch. This result also contradicts my hypothesis that common tone modulations would be the second most accurately recognized modulation condition, after direct modulations. In creating the hypothesis, we were conflicted. We theorized that the effect could work one of two ways. Either the common tone would serve as a guide into the new key, helping listeners track pitch region and identify when the new key presented itself, or the common tone would obliterate the memory of the old key so that listeners would be unsure of what the old key was when they heard the new key. It seems that the first of those was correct. Also, since processing time has also been a factor in previous work on both melodies and modulations, [@Thompson1989a; @Raman2017] and one of the surface features of the common tone modulation is a long tone that allows the listener time to process the material, it makes sense that this would allow for the most accurate responses.  
    With regard to key distance, (Figure \@ref(fig:mcgraph)) it's important to look at the results considering the harmonic content of the stimuli. First of all, for stimuli that did not change mode, key distance had a negligible effect on overall accuracy. However, for stimuli that did change mode, there was a large effect of key distance, namely that stimuli that changed mode were far more likely to have more accurate responses the less distance they modulated and less likely to have accurate responses the greater the key distance, with *A*' clustering around .8 for excerpts that modulated to the relative minor, and falling to about chance for the greatest key distances. It is worth noting that there were only a few stimuli that modulated to distant keys, and better balancing of stimuli across key distances may shed light on this effect. However, one interpretation of the results is that as modulation distance increases, mode change is more likely to act as a mask, obscuring the change in tonic, whereas stimuli that stay in the same mode maintain approximately the same level of accuracy regardless of key distance. Contrarily, stimuli that modulate shorter distances seem to get a boost from the mode change. This also makes sense in light of the current theories, that closer modulations are more easily recognized, and mode change serves as a cue to listeners that this has occured.  
    To incorporate some harmonic analysis into this discussion also helps illustrate this point: modulating to the relative minor is a fairly common modulation, and the harmonic distance between a given tonic and its relative minor is the smallest key distance possible. These stimuli that modulated to the relative minor were more likely to be recognized than even those that modulated to the dominant. At a glance, however, although the values predicted by the regression line for the non-modulating stimuli do not change very much, the graph seems to indicate greater dispersion in  *A*' values as key distance increases. These results of individual distances among stimuli are likely to come from factors not captured by this model, and would make for an interesting further investigation. This could examine effects of, for example,  where the modulation occurs relative to a phrase boundary, or the complexity of the harmonic language in a given stimulus.  
   With regard to response time (Figure \@ref(fig:rtgraph)), my initial hypothesis that highly trained listeners would react faster to the modulations is not borne out by the data. Instead, trained listeners responded the slowest, across all types of modulation. We think the effect in this case may be similar to the effect seen with pivot chord modulations. Untrained listeners responded quickly to the modulations, an effect which comes perhaps from their reliance on their subconsious process or instincts. The untrained listeners thus suffered a higher false-alarm rate and a lower overall accuracy, which is reflected in their overall *A*' scores. Highly trained listeners, on the other hand, had recruited cognitive resources in accessing this information and used the processing time necessary to wait for confirmation that what they were hearing was, in fact, a modulation, as opposed to a temporary tonicization, and were thus more accurate. The paradox here is that the highly-trained participants were worse overall at accurately identifying pivot chord modulations, an effect which really requires more looking into. The data also seem to suggest that listeners with moderate training performed in a similar manner to the untrained listeners.
   

```{r Stim_pca, include=FALSE}
library(corrplot)   #for correlation plots
library(PTCA4CATA)  #for various functions for PCA
library(ExPosition) #for regular PCA
library(InPosition) #for PCA inference battery
library(tidyverse)

colfunc <- colorRampPalette(c("firebrick4","gold","forestgreen","darkblue"))
c4part <- colfunc(9)
c4stim <- colfunc(13)


stimdats <- read.csv("stimdata.csv", header = TRUE)

s_vars <- c(12:14, 17:25, 27)
s_pcavars <- stimdats[ ,s_vars]
cor.s <- cor(s_pcavars)
corrplot(cor.s)

s_desvar <- stimdats$Type
stim_pca <- epPCA.inference.battery(s_pcavars, center = TRUE, scale = "SS1", DESIGN = s_desvar, graphs = FALSE)

s_scree <- PlotScree(ev = stim_pca$Fixed.Data$ExPosition.Data$eigs, 
                     p.ev = stim_pca$Inference.Data$components$p.vals,
                     plotKaiser = T, title = "Explained Variance, Stimuli")

Dim1 = 1
Dim2 = 2
stimH1 <- prettyHist(
           distribution = stim_pca$Inference.Data$components$eigs.perm[,Dim1], 
           observed = stim_pca$Fixed.Data$ExPosition.Data$eigs[Dim1], 
           xlim = c(0, as.numeric(stim_pca$Fixed.Data$ExPosition.Data$eigs[Dim1])*2), # needs to be set by hand
           breaks = 20,
           border = "white", 
           main = paste0("Permutation Test for Stimuli Eigenvalue ",Dim1),
           xlab = paste0("Eigenvalue ",Dim1), 
           ylab = "", 
           counts = FALSE, 
           cutoffs = c( 0.975))
#eigs1z <- recordPlot()
stimH1a <- prettyHist(
            distribution = stim_pca$Inference.Data$components$eigs.perm[,Dim2], 
           observed = stim_pca$Fixed.Data$ExPosition.Data$eigs[Dim2], 
           xlim = c(0, stim_pca$Fixed.Data$ExPosition.Data$eigs[Dim2]*2), # needs to be set by hand
           breaks = 20,
           border = "white", 
           main = paste0("Permutation Test for Stimuli Eigenvalue ",Dim2),
           xlab = paste0("Eigenvalue ",Dim2), 
           ylab = "", 
           counts = FALSE, 
           cutoffs = c( 0.975))


stim_fi <- createFactorMap(stim_pca$Fixed.Data$ExPosition.Data$fi,# data
                           title = "Modulation Type Row Factor Scores", # title of the plot
                           axis1 = 1, axis2 = 2, # which component for x and y axes
                           pch = 19, # the shape of the dots (google `pch`)
                           cex = 2, # the size of the dots
                           text.cex = 2.5, # the size of the text
                           col.points = stim_pca$Fixed.Data$Plotting.Data$fi.col, # color of the dots
                           col.labels = stim_pca$Fixed.Data$Plotting.Data$fi.col, # color for labels of dots
                           display.labels = FALSE,
                           alpha.points = .5
)

resCA <- stim_pca

stimfi_labs <- createxyLabels.gen(1,2,
                                  lambda = stim_pca$Fixed.Data$ExPosition.Data$eigs, 
                                  tau = round(stim_pca$Fixed.Data$ExPosition.Data$t), 
                                  axisName = "Component "
                                  
)

stim.fi.plot <- stim_fi$zeMap + stimfi_labs

stim.hulls <- MakeToleranceIntervals(stim_pca$Fixed.Data$ExPosition.Data$fi,
                                     design = s_desvar,
                                     col = unique(stim_pca$Fixed.Data$Plotting.Data$fi.col), p.level = .95
                                     )

stimmeans <- getMeans(stim_pca$Fixed.Data$ExPosition.Data$fi[ ,1:2], factor = s_desvar)

sboot <- Boot4Mean(stim_pca$Fixed.Data$ExPosition.Data$fi,
                   design = s_desvar,
                   niter = 1000
                   )

scis <- MakeCIEllipses(sboot$BootCube[ ,c(1,2), ], 
                       col = unique(stim_pca$Fixed.Data$Plotting.Data$fi.col),
                       p.level = .999
                       )

stimmean_fi <- createFactorMap(stimmeans,# data
                               title = "Modulation Type Row Factor Scores", # title of the plot
                               axis1 = 1, axis2 = 2, # which component for x and y axes
                               pch = 17, # the shape of the dots (google `pch`)
                               cex = 4, # the size of the dots
                               text.cex = 4, # the size of the text
                               col.points = unique(stim_pca$Fixed.Data$Plotting.Data$fi.col), # color of the dots
                               col.labels = "black", # color for labels of dots
                               display.labels = TRUE,
                               alpha.points = 1
)

stim.wm.plot <- stim.fi.plot + stimmean_fi$zeMap_dots + stimmean_fi$zeMap_text + stim.hulls + scis

stim_fj <- createFactorMap(stim_pca$Fixed.Data$ExPosition.Data$fj,# data
                           title = "Modulation Type Column Factor Scores", # title of the plot
                           axis1 = 1, axis2 = 2, # which component for x and y axes
                           pch = 19, # the shape of the dots (google `pch`)
                           cex = 2, # the size of the dots
                           text.cex = 4, # the size of the text
                           col.points = c4stim, # color of the dots
                           col.labels = c4stim, # color for labels of dots
                           display.labels = TRUE,
                           alpha.points = .5
)
stimfjplot <- stim_fj$zeMap + stimfi_labs

stim.loading <- cor(s_pcavars, stim_pca$Fixed.Data$ExPosition.Data$fi)
#colnames(stim.loading) <- rownames(stim.loading)

stimload.plot <- createFactorMap(stim.loading, 
                                 title = "Variable Loadings",
                                constraints = list(minx = -1, miny = -1,maxx = 1, maxy = 1),
                                col.points = c4stim,
                                col.labels = c4stim
)

sloadwcirc <- stimload.plot$zeMap + 
              addArrows(stim.loading, color = c4stim) +
              addCircleOfCor() + 
              xlab("Component 1") + ylab("Component 2") + 
              stimfi_labs

sbr <- stim_pca$Inference.Data$fj.boots$tests$boot.ratios
laDim = 1
sbr.001 <- PrettyBarPlot2(sbr[,laDim],
                        threshold = 2,
                        font.size = 5,
                   color4bar = c4stim,
                  ylab = 'Bootstrap ratios', 
                  line.col = "black"
                  ) + ggtitle("Bootstrap ratios", subtitle = paste0('Component ', laDim))
laDim = 2
sbr.002 <- PrettyBarPlot2(sbr[,laDim],
                        threshold = 2,
                        font.size = 5,
                   color4bar = c4stim,
                  ylab = 'Bootstrap ratios', 
                  line.col = "black"
                  ) + ggtitle("Bootstrap ratios", subtitle = paste0('Component ', laDim))

#  grid.arrange(
#    as.grob(sbr.001),
#    as.grob(sbr.002),
#    ncol = 1,nrow = 2,
#    top = text_grob("Barplots for variables", size = 18, face = "bold"))

```

## Exploratory Analyses

   A post hoc exploratory Principal Components Analysis (PCA) performed on the stimulus data also showed some interesting results. The PCA included as variables: average beats per minute (bpm) and bpm range, reaction window start, end, and length, excerpt length, *tcrit*^[tcrit is the precise moment of modulation, defined as the first appearance of the new tonic chord. For direct modulations, for example, this was equivalent to the beginning of the response window.], time (as a part of the whole excerpt) before and after tcrit^[All of the time variables listed here are measured in seconds.], key distance^[in arbitrary key distance units], date of composition, and *A*'. Permutation testing of the eigenvalues extracted by the PCA indicated that there were two significant dimensions extracted by the PCA. Dimension 1: $\lambda$ = `r round(stim_pca$Fixed.Data$ExPosition.Data$eigs[[1]], 2)`, $\tau$ = `r round(stim_pca$Fixed.Data$ExPosition.Data$t[[1]], 2)`, *p* < .01, Dimension 2:  $\lambda$ = `r round(stim_pca$Fixed.Data$ExPosition.Data$eigs[[2]], 2)`, $\tau$ = `r round(stim_pca$Fixed.Data$ExPosition.Data$t[[2]], 2)`, *p* < .01. The results are depicted graphically in Figure \@ref(fig:loadingplot). 
   

   
```{r loadingplot, include = FALSE, echo = FALSE, out.width='75%', fig.align = 'center'}

sloadwcirc
```

```{=latex}
\begin{figure}
  \caption{\\PCA variable loadings for exploratory analysis}%
  \label{fig:loadingplot}
    \begin{center}
      \includegraphics[width=.6\linewidth]{RTMParticle_files/figure-latex/loadingplot-1}
    \end{center}
  \caption*{\footnotesize \textit{Note.}  
Plot represents the loadings of the variables included in the PCA. In this graph, the angles between the arrows indicate the strength and direction of their correlation. Arrows that are close to 0 and 180 degrees apart are strongly correlated and anti-correlated, respectively. Arrows that approach 90 degrees are uncorrelated, i.e. share no information; are orthogonal. The length of the arrow relative to the edge of the circle indicates how much of the variance of that variable was extracted and explained by the PCA. Lambda is the eigenvalue and tau is the percentage of variance extracted by that eigenvalue.}
\end{figure}
```
   Much of the tempo information defined the first dimension (horizontal), with average bpm in the positive direction strongly anti-correlated with tcrit, reaction window start and end, and excerpt length in the negative direction. This makes sense in that music with a higher average bpm, i.e. that is faster, will take less time to perform, assuming similar excerpt length in measures. The second dimension (vertical) was dominated by, in the positive direction, reaction window as a percentage of the total excerpt time, composition date, key distance, and time before the reaction window. These were all anti-correlated with *A*' in the negative direction on the second dimension. Bootstrap testing for consistency indicated that all of these variables loaded consistently on the dimensions with which they are associated, with three exceptions. Bpm range and time after tcrit did not load consistently on either dimension, and reaction window length loaded consistently on both dimensions. Because *A*' is on the second dimension and the tempo data is on the first, we interpret that to mean that *A*' is, in fact, uncorrelated/orthogonal with tempo across excerpts. However, the variables with which *A*' is anti-correlated support the results that we found in the regression above, that greater key distance correlates with lower *A*'. A novel revelation from this analysis is that Date, reaction window length, reaction window length as a percentage of overall excerpt length, and bpm range are all correlated with one another and are anti-correlated with *A*'. Interpreting this requires inference on our part. We know that music that was written later, in the Romantic or late Classical periods, tends to have more complex harmonic language. More complex harmonic language has two results: longer reaction windows, due to, for example, extended dominant area and longer cadential phrases, and more obscured tonic. The fact that tempo variability, represented by bpm range, is also in the mix here, has a few possible implications. One the one hand, tempo tends to vary more at phrase boundaries, which would lead us to expect that greater tempo variablity would be associated with an increase in response accuracy as participants respond to the ebb and flow of tempo at phrase boundaries. However, in terms of performance practice, tempo tends to be more constant for music from the classical era relative to that of the romantic era. The fact that *A*' prime is anti-correlated with bpm range in this model supports the effect of date, and by extension, era, in how participants perceived the modulations. This also shows that, at least in this model, *A*' is orthogonal to reaction window length, meaning that the length of the reaction window had no effect on the participant's response accuracy. This is interesting for two specific reasons: it suggests that harmonic language and complexity does in fact play a role in our ability to perceive modulations, and that a longer reaction window did not help participants to be more accurate.  
   It's curious here, though, that *A*' was significantly higher for excerpts featuring a common-tone modulation, which is a convention that only arose in the romantic era.
   
   
## Future Directions
  Future work in this vein should include excerpts that are more evenly balanced across key distance. Because this experiment focused specifically on surface features and modulation types, the effects of key distance may be oversated by the small sample size of large key distance modulations. Other topical effects that future research should attempt to rule out are phrase boundary effects and effects of harmonic language and complexity. Additionally, it would be interesting to look at cross cultural studies into other musical idioms and cultures, and to look at different age groups to analyze the effects of passive exposure to music over the lifetime. Most interesting, however, would be research into the cognitive lag question that arises from the reaction time results  as well as the trained listeners' results on the pivot chord condition of the modulation type.

## Conclusions

In summary, we found evidence for the following conclusions:

1. Listeners, across training levels, track tonic region independently of surface features.
2. Training helps, but only when that training is at or approaches a professional level.
3. The most helpful surface feature is a sustained pitch that both provides reference and time to allow for listener comprehension.
4. Trained listeners take longer to respond, but are more accurate, likely because they are analyzing the harmonic information in real time. This specific justification requires more in-depth study.
5. Prior evidence regarding key distance and modulation perception, specifically cognitive lag in processing greater key distance, is supported.
6. Highly trained listeners seem to be able to access consciously the information regarding pitch set content and the specific function of each pitch in the set.

\pagebreak

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
\pagebreak

# Appendix A
\begin{center}
List of Recordings From Which Stimuli Were Excerpted
\end{center}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


Beethoven, L. v. (1801). Op. 18, No. 1, Mvt. 4: Allegro [Recorded by Quartetto Italiano]. On Complete String Quartets [CD]. London: Decca. (1996)  
Beethoven, L. v. (1801). Op. 18, No. 2, Mvt. 4: Allegro Molto quasi Presto [Recorded by Quartetto Italiano]. On Complete String Quartets [CD]. London: Decca. (1996)  
Brahms, J. (1861). Op. 18, Mvt. 1: Allegro ma non troppo [Recorded by Amadeus String Quartet]. On Quintette; Sextette [CD]. W. Germany: Philips. (1968)  
Brahms, J. (1865). Op. 36, Mvt. 4: Poco Allegro [Recorded by Amadeus String Quartet]. On Quintette; Sextette [CD]. W. Germany: Philips. (1968)  
Brahms, J. (1873). Op. 51, No. 1, Mvt. 3: Allegro molto moderato e comodo [Recorded by Quartetto Italiano]. On The Complete String Quartets; The Complete Clarinet Sonatas [CD]. New York, NY: Philips. (1997)  
Brahms, J. (1875). Op. 67, No. 3, Mvt. 1: Vivace [Recorded by Quartetto Italiano]. On The complete string quartets; The complete clarinet sonatas [CD]. New York, NY: Philips. (1997)  
Brahms, J. (1890). Op. 111, No. 2, Mvt. 4: Vivace ma non troppo presto [Recorded by Boston Symphony Chamber Players]. On String Quintets [CD]. New York, NY: Elektra/Nonesuch. (1984)  
Haydn, F. J. (1762). Op. 1, No. 1, Mvt. 1: Presto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1762). Op. 1, No. 1, Mvt. 3: Adagio [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1762). Op. 1, No. 1, Mvt. 5: Presto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 2, Mvt. 1: Allegro Molto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 2, Mvt. 2: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 3, Mvt. 1: Adagio [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 3, Mvt. 2: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 3, Mvt. 4: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 4, Mvt. 3: Adagio [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 4, Mvt. 4: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 1, Nos. 1- 4. [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1764). Op. 1, No. 5, Mvt. 3: Allegro Molto [Recorded by Kodaly QUartet]. On Haydn: String Quartets Op. 1, Nos. 5 and 6 ; Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1991)  
Haydn, F. J. (1764). Op. 1, No. 6, Mvt. 5: Presto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Nos. 5-8, Op. 1, Nos. 0 and 6, and Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1765). Op. 2, No 4, Mvt. 4: Menuetto: Allegretto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 42 and Op. 2, Nos 4 and 6 [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1765). Op. 2, No. 1, Mvt. 2: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Nos. 5-8, Op. 1, Nos. 0 and 6, and Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1765). Op. 2, No. 2, Mvt. 1: Allegro Molto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Nos. 5-8, Op. 1, Nos. 0 and 6, and Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1765). Op. 2, No. 2, Mvt. 4: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Nos. 5-8, Op. 1, Nos. 0 and 6, and Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1992)  
Haydn, F. J. (1765). Op. 2, No. 3, Mvt. 1: Allegro Molto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 2, Nos. 3 and 5 [CD]. Hong Kong: Naxos. (2003)  
Haydn, F. J. (1765). Op. 2, No. 4, Mvt. 3: Adagio non troppo [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 42 and Op. 2, Nos 4 and 6 [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1766). Op. 2, No. 6, Mvt. 5: Presto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 42 and Op. 2, Nos 4 and 6 [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1769). Op. 9, No. 1, Mvt. 1: Moderato [Recorded by Kodaly Quartet]. On Haydn: String Quartets, Op. 9, Nos. 1, 3, and 4 [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1769). Op. 9, No. 1, Mvt. 3: Adagio [Recorded by Kodaly Quartet]. On Haydn: String Quartets, Op. 9, Nos. 1, 3, and 4 [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1769). Op. 9, No. 2, Mvt. 3: Adagio Cantabile [Recorded by Kodaly Quartet]. On Haydn: String Quartets, Op. 9, Nos. 1, 3, and 4 [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1769). Op. 9, No. 3, Mvt. 1: Allegro Moderato [Recorded by Kodaly Quartet]. On Haydn: String Quartets, Op. 9, Nos. 1, 3, and 4 [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1772). Op. 20, No. 2, Mvt. 1: Moderato [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 20, Nos. 1- 3, 'Sun Quartets' [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1772). Op. 20, No. 6, Mvt 2: Adagio cantabile [Recorded by Kodaly Quartet]. On Haydn: String Quartets, Op. 20, Nos. 4-6, 'Sun Quartets' [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1781). Op. 33, No. 2, Mvt. 1: Allegro Moderato, cantabile [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 33, Nos. 1, 2 and 5 [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1782). Op. 33, No. 5, Mvt. 2: Largo e cantabile [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 33, Nos. 1, 2 and 5. [CD]. Hong Kong: Naxos. (1994)  
Haydn, F. J. (1790). Op. 64, No. 4, Mvt. 3: Adagio [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 64, Nos. 4-6. [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1790). Op. 64, No. 5, Mvt. 3: Minuet [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 64, Nos. 4-6. [CD]. Hong Kong: Naxos. (1993)  
Haydn, F. J. (1790). Op. 76, No. 1, Mvt. 1: Allegro con Spirito [Recorded by Kodaly Quartet]. On String quartets, op. 76, nos. 1-3 [CD]. Hong Kong: Naxos. (1990)  
Hofstetter, R. (1777). Op. 3, No. 1, Mvt. 1: Allegro Molto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 2, Nos. 3 and 5, Op. 3, Nos. 1 and 2 [CD]. Franklin, Tenn: Naxos. (2006)  
Hofstetter, R. (1777). Op. 3, No. 1, Mvt. 3: Andantino Grazioso [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 2, Nos. 3 and 5, Op. 3, Nos. 1 and 2 (attrib. Hoffstetter) [CD]. Franklin, Tenn: Naxos. (2006)  
Hofstetter, R. (1777). Op. 3, No. 1, Mvt. 4: Presto [Recorded by Kodaly Quartet]. On String Quartets Op. 2, Nos. 3 and 5, Op. 3, Nos. 1 and 2 (attrib. Hoffstetter) [CD]. Franklin, Tenn: Naxos. (2006)  
Hofstetter, R. (1777). Op. 3, No. 3, Mvt. 3: Menutetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 3, Nos. 3 - 6 [CD]. Hong Kong: Naxos. (2002)  
Hofstetter, R. (1777). Op. 3, No. 5, Mvt. 1: Presto [Recorded by Kodaly Quartet]. On String Quartets: Op. 1, Nos. 5 and 6; Op. 2, Nos. 1 and 2 [CD]. Hong Kong: Naxos. (1991)  
Hofstetter, R. (1777). Op. 3, No. 6, Mvt. 3: Menuetto [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 3, Nos. 3 - 6 [CD]. Hong Kong: Naxos. (2002)  
Hofstetter, R. (1777). Op. 3, No. 6, Mvt. 4: Scherzando [Recorded by Kodaly Quartet]. On Haydn: String Quartets Op. 3, Nos. 3 - 6 [CD]. Hong Kong: Naxos. (2002)  
Mozart, W. A. (1786). String Quartet No. 20 in D Major, KV 499 [Recorded by Quatuor Talich]. On Inte´grale des quatuor [CD]. France: Calliope. (1993)  
Mozart, W. A. (1790). String Quartet No. 22 in Bb Major, KV 589 [Recorded by Quatuor Talich]. On Inte´grale des quatuors [CD]. France: Calliope. (1993)  
Schubert, F (1812). String Quartet in Bb Major, D. 36, Mvt. 2: Andante [Recorded by Melos Quartett]. On The String Quartets [CD]. Hamburg: Deutsche Grammophon. (1999)  
Schubert, F. (1814). String Quartet in Bb Major, D. 112, Mvt. 1: Allegro ma non troppo [Recorded by Wiener Konzerthausquartett]. On Les quatuors a` cordes [CD]. Paris: Universal Music, 1998. (1998)  
Schubert, F. (1820). String Quartet Movement in C minor, D. 703 [Recorded by Melos Quartett]. On The String Quartets [CD]. Hamburg: Deutsche Grammophon. (1999)  
Schubert, F. (1828). Quintet in C Major, Op. 163, D. 956, Mvt. 1: Allegro ma non troppo [Recorded by Bernard Greenhouse, Juilliard String Quartet]. On Quintet in C Major, Op. 163, D. 956 [CD]. New York, NY: CBS. (1988)  
Schubert, F. (1828). Quintet in C Major, Op. 163, D. 956, Mvt. 2: Adagio [Recorded by Bernard Greenhouse, Juilliard String Quartet]. On Quintet in C Major, Op. 163, D. 956 [CD]. New York, NY: CBS. (1988)  
Schubert, F. (1828). Quintet in C Major, Op. 163, D. 956, Mvt. 3: Scherzo [Recorded by Bernard Greenhouse, Juilliard String Quartet]. On Quintet in C Major, Op. 163, D. 956 [CD]. New York, NY: CBS. (1988)  


\endgroup
\pagebreak

# Appendix B
\begin{center}
List of Scores Consulted for Analysis
\end{center}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

Beethoven, L. v. (1937). Op. 18, No. 1. New York: E. F. Kalmus Orchestra Scores. (Original work published 1801)  
Beethoven, L. v. (1937). Op. 18, No. 2. New York: E. F. Kalmus Orchestra Scores. (Original work published 1801)  
Brahms, J. (1927). Op. 18. Leipzig: Breitkopf & Härtel. (Original work published 1861) 
Brahms, J. (1927). Op. 36. Leipzig: Breitkopf & Härtel. (Original work published 1865)  
Brahms, J. (1927). Op. 51. Leipzig: Breitkopf & Härtel. (Original work published 1873)  
Brahms, J. (1927). Op. 67. Leipzig: Breitkopf & Härtel. (Original work published 1875)  
Brahms, J. (1927). Op. 111, No. 2. Leipzig: Breitkopf & Härtel. (Original work published 1890)  
Haydn, F. J. (1845). Op. 1, No. 1. Berlin: Trautwein. (Original work published 1762)  
Haydn, F. J. (1845). Op. 1, No. 2. Berlin: Trautwein. (Original work published 1764)  
Haydn, F. J. (1845). Op. 1, No. 3. Berlin: Trautwein. (Original work published 1764)  
Haydn, F. J. (1845). Op. 1, No. 4. Berlin: Trautwein. (Original work published 1764)  
Haydn, F. J. (1845). Op. 1, No. 5. Berlin: Trautwein. (Original work published 1764)  
Haydn, F. J. (1845). Op. 1, No. 6. Berlin: Trautwein. (Original work published 1764)  
Haydn, F. J. (1845). Op. 2, No. 4. Berlin: Trautwein. (Original work published 1765)  
Haydn, F. J. (1845). Op. 2, No. 1. Berlin: Trautwein. (Original work published 1765)  
Haydn, F. J. (1845). Op. 2, No. 2. Berlin: Trautwein. (Original work published 1765)  
Haydn, F. J. (1845). Op. 2, No. 3. Berlin: Trautwein. (Original work published 1765)  
Haydn, F. J. (1845). Op. 2, No. 4. Berlin: Trautwein. (Original work published 1765)  
Haydn, F. J. (1845). Op. 2, No. 6. Berlin: Trautwein. (Original work published 1766)  
Haydn, F. J. (1845). Op. 9, No. 1. Berlin: Trautwein. (Original work published 1769)  
Haydn, F. J. (1845). Op. 9, No. 1. Berlin: Trautwein. (Original work published 1769)  
Haydn, F. J. (1930). Op. 9, No. 2. Leipzig: Ernst Eulenburg. (Original work published 1769)  
Haydn, F. J. (1845). Op. 9, No. 3. Berlin: Trautwein. (Original work published 1769)  
Haydn, F. J. (1930). Op. 20, No. 2. Leipzig: Ernst Eulenberg. (Original work published 1772)  
Haydn, F. J. (1930). Op. 20, No. 6. Leipzig: Ernst Eulenburg. (Original work published 1772)  
Haydn, F. J. (1930). Op. 33, No. 2. Leipzig: Ernst Eulenburg. (Original work published 1781)  
Haydn, F. J. (1930). Op. 33, No. 5. Leipzig: Ernst Eulenburg. (Original work published 1782)  
Haydn, F. J. (1968). Op. 64, No. 4. Moscow: State Publishers Music. (Original work published 1790)  
Haydn, F. J. (1968). Op. 64, No. 5. Moscow: State Publishers Music. (Original work published 1790)  
Haydn, F. J. (1968). Op. 76, No. 1. Moscow: State Publishers Music. (Original work published 1790)  
Hofstetter, R. (1845). Op. 3, No. 1. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 1. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 1. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 3. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 5. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 6. Berlin: Trautwein. (Original work published 1777)  
Hofstetter, R. (1845). Op. 3, No. 6. Berlin: Trautwein. (Original work published 1777)  
Mozart, W. A. (1882). String Quartet No. 20 in D Major, KV 499. Leipzig: Breitkopf and Härtel. (Original work published 1786)  
Mozart, W. A. (1882). String Quartet No. 22 in Bb Major, KV 589. Leipzig: Breitkopf & Härtel. (Original work published 1790)  
Schubert, F. (1973). String Quartet in Bb Major, D. 36. New York, NY: Dover Publications. (Original work published 1812)  
Schubert, F. (1965). String Quartet in Bb Major, D. 112. New York, NY: Dover Publications. (Original work published 1814)  
Schubert, F. (1965). String Quartet Movement in C minor, D. 703. New York, NY: Dover Publications. (Original work published 1820)  
Schubert, F. (1965). Quintet in C Major, Op. 163, D. 956. New York, NY: Dover Publications. (Original work published 1828)  
  
\endgroup
